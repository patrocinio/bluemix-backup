<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>Backup & Restore Bluemix Dedicated Documentation</title>

    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight, .highlight .w {
  color: #f8f8f2;
  background-color: #272822;
}
.highlight .err {
  color: #151515;
  background-color: #ac4142;
}
.highlight .c, .highlight .cd, .highlight .cm, .highlight .c1, .highlight .cs {
  color: #505050;
}
.highlight .cp {
  color: #f4bf75;
}
.highlight .nt {
  color: #f4bf75;
}
.highlight .o, .highlight .ow {
  color: #d0d0d0;
}
.highlight .p, .highlight .pi {
  color: #d0d0d0;
}
.highlight .gi {
  color: #90a959;
}
.highlight .gd {
  color: #ac4142;
}
.highlight .gh {
  color: #6a9fb5;
  background-color: #151515;
  font-weight: bold;
}
.highlight .k, .highlight .kn, .highlight .kp, .highlight .kr, .highlight .kv {
  color: #aa759f;
}
.highlight .kc {
  color: #d28445;
}
.highlight .kt {
  color: #d28445;
}
.highlight .kd {
  color: #d28445;
}
.highlight .s, .highlight .sb, .highlight .sc, .highlight .sd, .highlight .s2, .highlight .sh, .highlight .sx, .highlight .s1 {
  color: #90a959;
}
.highlight .sr {
  color: #75b5aa;
}
.highlight .si {
  color: #8f5536;
}
.highlight .se {
  color: #8f5536;
}
.highlight .nn {
  color: #f4bf75;
}
.highlight .nc {
  color: #f4bf75;
}
.highlight .no {
  color: #f4bf75;
}
.highlight .na {
  color: #6a9fb5;
}
.highlight .m, .highlight .mf, .highlight .mh, .highlight .mi, .highlight .il, .highlight .mo, .highlight .mb, .highlight .mx {
  color: #90a959;
}
.highlight .ss {
  color: #90a959;
}
    </style>
    <link href="stylesheets/screen.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print.css" rel="stylesheet" media="print" />
      <script src="javascripts/all.js"></script>
  </head>

  <body class="index" data-languages="[&quot;javascript&quot;,&quot;java&quot;]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar.png" />
      </span>
    </a>
    <div class="tocify-wrapper">
      <img src="images/logo.png" />
        <div class="lang-selector">
              <a href="#" data-language-name="javascript">Node</a>
              <a href="#" data-language-name="java">Java</a>
        </div>
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <div id="toc">
      </div>
    </div>
    <div class="page-wrapper">
        <div id="expand-collapse-code" role="complementary" aria-label="expand-collapse-code">
          <button class="showHideButton" id="hideCodeButton" title="hide code column"><label for="hideCodeButton">&nbsp;</label></button>
          <button class="showHideButton" id="showCodeButton" title="show code column"><label for="showCodeButton">&nbsp;</label></button>
        </div>
      <div class="dark-box"></div>
      <div class="content">
        <h1 id="overview">Overview</h1>

<p>Welcome to the Backup &amp; Restore documentation for Bluemix Dedicated. </p>

<p>Not every service on Bluemix has a backup strategy because not every service manages data to be backed up.  What’s more, given the approach on Bluemix of letting each product operate in a way consistent with the standard operational principles of that underlying service, the backup and restore procedures for each can differ.  The following services have specific backup and restore or high availability procedures.</p>

<p><strong>DashDB</strong> – For the DashDB Managed Service only, encrypted backups on the full database are done daily. The last two backups are retained. Point-in-time restores are not available.</p>

<p><strong>Apache Spark</strong> – You can reuse an external SoftLayer Object Storage account in Apache Spark on Bluemix to store data redundantly.  Softlayer Object storage is highly available and replicated.</p>

<p><strong>DB2 on Cloud</strong> -  DB2 on Cloud allows you to perform regular backups of your data to remote cloud storage  (IBM Softlayer Object Storage or Amazon S3) using the BACKUP command.</p>

<p>Some Bluemix services (such as the Time Series Database) do not have customer-accessible backup or restore procedures.  The backup and restore procedures of third-party services such as ElephantSQL are the responsibility of the service provider.</p>

          <h1 id="compose">Compose</h1>

<p><strong>Relationship to Bluemix</strong></p>

<p>The Compose DBaaS is an IBM company. It is not directly associated with Cloud Foundry (the open-source project behind IBM Bluemix). Compose provides <em>Databases-as-a-Service</em> (aka: <em>DBaaS</em>) for popular open source databases. The service uses a Bluemix Service Broker to bind the Compose service to the <a href="https://console.ng.bluemix.net/docs/admin/index.html#oc_catalog">Bluemix Platform</a>.</p>

          <!--- CLIENT SPECIFIC DATA --->

<h2 id="backup">Backup</h2>

<p>By default, the Compose DBaaS does <a href="#scheduled-backup">scheduled backups</a> for every database deployment.</p>

<p><strong>Application Owners</strong><br>
As an application owner, scheduled backup does provide a safety net that will generally suffice for applications that make use of a single database. There are <a href="#manual">use cases</a>, such as an <a href="#polyglot-persistence">application that uses multiple databases</a>, that may necessitate backing up manually.  </p>

<p><strong>Bluemix Administrators</strong><br>
As an administrator of the Bluemix environment, Compose&rsquo;s <a href="#scheduled">**Scheduled Backups</a> ensure that databases are being backed up without requiring additional administrative attention. There are occasions where a Bluemix Administrator may want to create a backup [<strong>On Demand</strong>] or have the need to capture a consistent state of the entire system. These topics are covered below.</p>

<h3 id="design">Design</h3>

<p>Backups done by Compose were designed to minimize, or eliminate, any interruption to the database performance. To achieve this, backups are run against a non-active member of a deployment such that they will not be running against a node that serves queries.</p>

<p>For example, when a new Compose MongoDB deployment is created, part of the deployment is the addition of a full (but hidden) third member to the MongoDB replica set. The hidden member node is paused from accepting writes by sending it a <code class="prettyprint">db.fsyncLock()</code> which flushes out all the pending write operations and locks the database.</p>

<aside class='success'>
This lock is on the hidden member and will not impact the other cluster members.
</aside>

<!--- CLIENT SPECIFIC DATA --->

<!--
  > Kaiser Permanente uses the IBM Object Store (formerly Cleversafe) in Softlayer&rsquo;s Dallas09 datacenter, in lieu of a publicly hosted object store implementation
  &ndash;>

<p>Once the <code class="prettyprint">fsyncLock</code> is engaged, an OS operation copies the data files from the XFS filesystem and transmits the data files to an s3 compatible object store securely over https and encrypted at rest.</p>

<aside class='notice'>
The default storage location that Compose uses is Amazon&rsquo;s S3.
</aside>

<p>When this copy is completed, we send a <code class="prettyprint">db.fsyncUnlock()</code> and release the hidden member. It then catches up with the replication state of cluster members and rapidly returns the replica set to complete consistency.</p>

<p>Compose keeps track of the backups through an internal database which holds on to the one <a href="#on-demand">on-demand backup</a> and the <a href="#scheduled">daily, weekly and monthly backup</a> sets. From that database, Compose serves backup metadata for archive location, timestamps, etc for download or into the data directory of a new deployment for a restore.</p>

<p>Compose stores the data files from the filesystem, rather than a dump, for all databases except <code class="prettyprint">etcd</code> and <code class="prettyprint">RethinkDB</code>.  This tactic ensures it is easier and faster to bring up an entire database.</p>

<p>Backup infrastructure is not exposed to any customer on any IBM platforms.</p>

<p>Elasticsearch, Postgres, and Redis all use strategy similar to stopping writes to a hidden member and capturing the data files, as how they reside on disk. RethinkDB and etcd use specific tools (ie: <code class="prettyprint">rethinkdb dump</code> and <code class="prettyprint">etcdtool</code>, respectively) that generate a JSON formatted backup file, rather than directly using the filesystem.  In all cases, the files created for backup are transmitted via HTTPS to be archived in an S3-compatible object storage.</p>

<h3 id="security">Security</h3>

<p>Compose employees are granted access only to those systems and system features deemed necessary for the completion of their role as part of a &ldquo;least required&rdquo; access provisioning methodology. These access levels are reviewed regularly to ensure as roles change, access remains as limited as absolutely necessary.</p>

<p>Compose enlists the services of globally-recognized information security firms to assist in third-party security audits, recommendations in best practices, and application security guideline development. Please see Compose&rsquo;s <a href="https://help.compose.com/docs/security#section-disclosure-and-auditing">security documentation</a> for further information.</p>

<h4 id="physical">Physical</h4>

<!--- CLIENT SPECIFIC DATA --->

<!--
  > For Kaiser Permanente, Compose databases are hosted in a dedicated environment  [SoftLayer&rsquo;s Dallas09 datacenter](http://www.softlayer.com/data-centers). Backups are hosted in a dedicated IBM Object Store (formerly Cleversafe) instance, also hosted in the SoftLayer Dallas09 datacenter.  
  SoftLayer Dallas09 datacenter uses Proximity badge and Biometric systems to secure physical access to the datacenter.  SoftLayer provides it&rsquo;s [most recent SOC 3 report](http://static.softlayer.com/sites/default/files/assets/page/softlayer-soc_3-103115.pdf). Further information can be found at the [SoftLayer compliance website](http://www.softlayer.com/compliance).
  &ndash;>

<p>Compose hosts documentation regarding <a href="https://help.compose.com/docs/security#section-physical-plant-controls">Physical Security on the support website</a></p>

<h4 id="network">Network</h4>

<!--- CLIENT SPECIFIC DATA --->

<!--
  > Kaiser Permanente&rsquo;s Compose hosted databases are only accessible from the Kaiser Permanente network with access to the `169.45.121.x` IP range.
  &ndash;>

<p>Compose servers transmit backup data via an encrypted HTTPS using industry standard SSL/TLS <em>high</em> (&gt;128-bit) encryption. Asymmetrical Key signatures are 2048-bit encryption and symmetrical connection encryption is 256-bit.</p>

<p>Data is never transmitted over a network that has visibility into adjacent user data. Compose utilizes network Access Control Lists as well as <a href="http://en.wikipedia.org/wiki/IEEE_802.1Q">802.1q VLAN</a> segmentation to ensure each customer environment is entirely autonomous.</p>

<h4 id="filesystem-storage">Filesystem/Storage</h4>

<!--- CLIENT SPECIFIC DATA --->

<!--
  > Kaiser Permanente backup data from Compose is stored in the IBM Object store on a 12-node storage cluster for redundancy.
  &ndash;>

<p>All Compose database servers use the <a href="http://xfs.org/index.php/XFS_FAQ">XFS</a> filesystem secured with <a href="https://gitlab.com/cryptsetup/cryptsetup/">LUKS</a>, the open source disk encryption software. Backups are encrypted and stored in an S3-compatible object store.</p>

<aside class='notice'>
Further information about Compose&rsquo;s <a href='https://help.compose.com/docs/security#section-system-security'>systems security measures can be found on their website</a>.
</aside>

<h3 id="scheduled">Scheduled</h3>

<p>When a new database system is created in Compose, this is referred to as a <em>&ldquo;Deployment&rdquo;</em>.  A database <em>deployment</em> can be one of:</p>

<ul>
<li><a href="https://www.compose.com/mongodb/">MongoDB</a></li>
<li><a href="https://www.compose.com/elasticsearch">ElasticSearch</a></li>
<li><a href="https://www.compose.com/rethinkdb/">RethinkDB</a></li>
<li><a href="https://www.compose.com/redis">Redis</a></li>
<li><a href="https://www.compose.com/postgresql">Postgres</a></li>
</ul>

<p>For every deployment, Compose will <strong>automatically</strong> backup every single deployment on a set schedule.</p>

<aside class="warning">
Compose recently launched <a href="https://enterprise.compose.com/" alt="Compose Enterprise">Enterprise Clusters</a> targeted at applications in public clouds.  At the time of this writing, Enterprise Clusters is only available for Amazon Web Services and Google Cloud Platform. Enterprise Clusters are not necessary for Bluemix Dedicated Customers.
</aside>

<aside class='notice'>
There are several frequently asked questions about Compose backups can be found <a href='https://help.compose.com/docs/compose-backups'>on the Compose help website</a>
</aside>

<h4 id="intervals">Intervals</h4>

<p>For each deployment, Compose will <strong>automatically</strong> schedule backup each deployment on a set schedule. The time of day when the backup operation runs is not configurable, nor is the backup data retention policy. No additional configuration is possible or required.</p>

<table><thead>
<tr>
<th>Schedule</th>
<th>Retention</th>
</tr>
</thead><tbody>
<tr>
<td>Daily</td>
<td>Last 7 daily backups</td>
</tr>
<tr>
<td>Weekly</td>
<td>Last 4 weekly backups</td>
</tr>
<tr>
<td>Monthly</td>
<td>Last 3 monthly backups</td>
</tr>
</tbody></table>

<aside class='notice'>
Please see <a href='https://help.compose.com/docs/compose-backups#changing-schedule-and-retention'>the Compose FAQ for more information about the Compose backup schedule and retention policy.</a>
</aside>

<h4 id="accessibility">Accessibility</h4>

<p><strong>Backup Dashboard</strong>
Scheduled (and On-Demand) Backups are accessible through the Compose Backup dashboard.</p>

<p>To access the Compose Backup Dashboard:</p>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get to the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose Deployment view, click <strong>Backups</strong>, which is the first icon on the left-hand navigation.</p>

<p><img alt="Compose: Backups" src="images/compose/backup/backups.png" /><br>
5. From the Compose Backup dashboard, choose a backup to download by clicking the green icon to the right of the backup timestamp.</p>

<p><img alt="Compose: Download" src="images/compose/backup/download.png" />  </p>

<p>This will result in a tar file being streamed to you, named by the deployment name, timestamp, and backup type (ie: <code class="prettyprint">daily</code>).</p>

<aside class='warning'>
ElasticSearch backups are not available to download.
</aside>

<aside class='notice'>
<a href='https://help.compose.com/docs/compose-backups#how-do-compose-backups-work'>The Compose service ensures that deleted deployments automatically create a Deprovisioned Backup</a>; however these are only accessible by contacting support.
<br>
It is the best practice is to also <a href="#on-demand">create an On-Demand Backup</a>, <a href="#accessibility">download it</a> and archive locally before deleting a deployment.
</aside>

<p><strong>API</strong></p>

<p>Accessing Backups via the API is only available for Classic MongoDB Deployments, which are not advised for production use.  The API will be deprecated in the future. Please see <a href="#api-support">API Support</a> and <a href="https://www.compose.com/articles/how-to-automatically-compose-your-backups/">a blog post from August 2014</a> for more information.</p>

<h4 id="failure">Failure</h4>

<p>If an scheduled backup fails, the Compose service <strong>will</strong> notify the Compose support team; however, it <strong>will not</strong> notify the users associated with the account. Therefore, it is <strong>highly recommended</strong> that users of Compose  verify that scheduled backups are successfully.  Failed backups are rare, but are visible in the Backup view of a deployment.</p>

<h4 id="performance">Performance</h4>

<p>Backups done by Compose were designed to minimized, or eliminate, any interruption to the database performance. To achieve this, backups are run against a non-active member of a deployment such that they will not be running against a node that serves queries. See the <a href="#design">Design section</a> for details.</p>

<h4 id="locations">Locations</h4>

<!--- CLIENT SPECIFIC DATA --->

<!--
  >In Kaiser Permanente&rsquo;s dedicated Compose, all databases are hosted in [Softlayer&rsquo;s Dallas09 datacenter](http://www.softlayer.com/data-centers) and all backups are store in a IBM Object Store (formerly Cleversafe) instance dedicated to Kaiser Permanente, also located in Softlayer&rsquo;s Dallas09 datacenter.  
  &ndash;>

<p>In a dedicated environment, Compose will store backups of database deployments in one of two possible locations:</p>

<ol>
<li>By default, an Amazon S3 location closest to the dedicated deployment, OR</li>
<li>By customer choice, an IBM Object Store location specified at the time of licensing.</li>
</ol>

<aside class='warning'>
At the time of this writing, IBM Object Store doesn&rsquo;t support encryption, so backups might not be encrypted if this option is selected.
</aside>

<p>For those interested in learning about the public Compose datacenter availability, please see the <a href="https://help.compose.com/docs/compose-datacenter-availability">Compose help website</a>.</p>

<h4 id="data-types">Data Types</h4>

<p>Compose&rsquo;s Scheduled backups take file-system level snapshots of the database&rsquo;s data files (see <a href="#design">Design</a>). Therefore, regardless of the datatype, the data will be captured by the Compose backup operation.</p>

<p>The notable exceptions are RethinkDB and etcd (see <a href="#design">Design</a>), which only accept JSON documents (which are fundamentally strings), so the backup will contain all data stored.</p>

<h5 id="limitations">Limitations</h5>

<p><strong>Consistent Snapshot of Multiple Databases</strong>
Compose does not have an interface to snapshot all databases at once. Coordination across multiple databases in a single application or across all Organizations in a dedicated Bluemix, is not possible through the Compose provided tools. This can be done <a href="#manual">manually</a>; however conducting backups from outside of Compose will likely negatively impact the database&rsquo;s performance.</p>

<p><strong>Multiple Account Visibility</strong>
  At the time of this writing, it is not possible for an administrator to view all databases or status across all Compose accounts in a dedicated Compose environment. The Compose roadmap will likely add this feature in a future release.</p>

<h3 id="on-demand">On Demand</h3>

<p>In the case where an Bluemix Administrator or Application Owner needs to create a backup manually, the Compose web interface makes this possible through the click of a button.</p>

<aside class='success'>
Despite the Compose Backup dashboard&rsquo;s statement that On-Demand Backups are limited to only 10 per week, <strong>this limitation has been removed</strong>.  The Compose Backup dashboard&rsquo;s content will be corrected.
</aside>

<p><strong>Accessibility</strong><br>
On-Demand backups are accessible in the same manner as scheduled backups.  Please see <a href="#accessibility">Scheduled Backup Accessibility section</a>.</p>

<h4 id="instructions">Instructions</h4>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get to the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose Deployment view, click <strong>Backups</strong>, the first icon on them the left-hand navigation.</p>

<p><img alt="Compose: Backups" src="images/compose/backup/backups.png" /><br>
5. From the Compose Backup view, click the <strong>Back up now</strong> button</p>

<p><img alt="Compose: Back up now" src="images/compose/backup/ondemand/backupnow.png" /><br>
6. Compose will redirect to the Jobs view, with a notice saying <strong>Performing Backup Now</strong></p>

<p><img alt="Compose: Performing Job" src="images/compose/backup/ondemand/running.png" /><br>
7. When the on-demand backup job completes successfully, the Jobs view will display the message <strong>All operations have completed successfully</strong>  with the date-time stamp.</p>

<p><img alt="Compose: Complete" src="images/compose/backup/ondemand/complete.png" /></p>

<h5 id="failure">Failure</h5>

<p>When performing an <strong>On-Demand Backup</strong>, in rare circumstances the backup operation may fail.</p>

<p><img alt="Compose: Backup Failure" src="images/compose/backup/ondemand/failure.png" />  </p>

<p>In the case of a on-demand backup failure, the IBM Compose support team will be automatically notified.  Although Compose Support is notified, the best course of action is to open a new support ticket to track the failure.</p>

<aside class="notice">
In the event of a failure, the Compose UI will display the failure, but will not send a notification to the users on the account.  For on-demand backups, always verify the backup completes successfully.
</aside>

<h4 id="limitations">Limitations</h4>

<p>Backup jobs are serialized per deployment.  This means that only one (1) On-Demand backup job for a deployment can execute at a time. Issuing a request for a second backup job will cancel the first backup job.</p>

<h3 id="manual">Manual</h3>

<p><em>&ldquo;Manual backups&rdquo;</em>, in the scope of this documentation, means using the database-vendor or open-source community supplied tools to <em>&ldquo;dump&rdquo;</em> data via network to a target location. The toolset likely include <code class="prettyprint">mongodump</code>, <code class="prettyprint">pg_dump</code>, <code class="prettyprint">redis dump</code>, <code class="prettyprint">etcdtool</code>, <code class="prettyprint">elasticdump</code>, and <code class="prettyprint">rethinkdb dump</code>. Additionally, this section of the documentation applies to any other tool that uses the database&rsquo;s APIs in order to backup.</p>

<p>Doing a manual backup from a Compose database is a calculated risk. Backing up manually may provide the end user of the database with more control; however it will comes at the cost of impacting the database&rsquo;s performance. This is because the accessible nodes are also responsible for responding to application requests as well.</p>

<aside class='warning'
Creating a backup manually <b> will negatively impact performance.</b>
</aside>

<h4 id="postgres">Postgres</h4>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get to the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target deployment to access the Overview dashboard.  Note the administrator credentials and database name.</p>

<p><img alt="Bluemix: Postgres" src="images/compose/backup/postgres/overview.png" />
<img alt="Bluemix: Postgres" src="images/compose/backup/postgres/credentials.png" /></p>

<h5 id="single-postgres-database">Single Postgres Database</h5>

<p>To create a back up of a single database, use the  <a href="https://www.postgresql.org/docs/current/static/app-pgdump.html"><code class="prettyprint">pg_dump</code></a> tool on your localhost, execute the following command</p>

<!--- CLIENT SPECIFIC DATA --->

<!--- TODO: ADD KP SPECIFIC EXAMPLE WHEN COMPOSE IS ACCESSIBLE --->

<p><code class="prettyprint">pg_dump -U admin -h bluemix-test1-ibm-305.2.compose.direct -p 14047 -d exampledb &gt; db.sql</code></p>

<h5 id="all-postgres-databases">All Postgres Databases</h5>

<p>To create a back up of all databases in a particular Postgres deployment, use the  <a href="https://www.postgresql.org/docs/current/static/app-pg-dumpall.html"><code class="prettyprint">pg_dumpall</code></a> tool on your localhost, execute the following command:</p>

<!--- CLIENT SPECIFIC DATA --->

<!--- TODO: ADD KP SPECIFIC EXAMPLE, WHEN COMPOSE IS ACCESSIBLE --->

<p><code class="prettyprint">pg_dump -U admin -h bluemix-test1-ibm-305.2.compose.direct -p 14047 &gt; db.out</code></p>

<h4 id="mongodb">MongoDB</h4>

<aside class='warning'>
Manual backups of MongoDB should be used as a last resort, because backups will impact the performance of a running database.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get to the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target MongoDB deployment to access the Overview dashboard.  </p>

<p><img alt="Bluemix: Deployment Databases" src="images/compose/backup/mongo/databases.png" />
5. Select a Mongo database and then select the <code class="prettyprint">Users</code> view.</p>

<p><img alt="Bluemix: Mongo Users" src="images/compose/backup/mongo/users.png" />
6. Verify that you have an existing user with <code class="prettyprint">readOnly</code>, <code class="prettyprint">readWrite</code>, or <code class="prettyprint">dbOwner</code>.  If not, add a user.</p>

<p><img alt="Bluemix: Add Users" src="images/compose/backup/mongo/add_users.png" />
7. With valid user credentials on your target database, follow the instructions using <code class="prettyprint">mongodump</code> as described in MongoDB&rsquo;s <a href="https://docs.mongodb.com/manual/tutorial/backup-sharded-cluster-with-database-dumps/">Back Up a Sharded Cluster with Database Dumps</a> documentation.</p>

<aside class='notice'>
Compose operates MongoDB as a sharded cluster; thus <code>mongodump</code> will establish a connection to <code>mongos</code> nodes. More information   can be found at <a href="https://docs.mongodb.com/manual/tutorial/backup-sharded-cluster-with-database-dumps/">MongoDB&rsquo;s documentation</a>
</aside>

<aside class='warning'>
To capture a point-in-time backup from a sharded cluster you must stop all writes to the cluster. On a running production system, you can only capture an approximation of point-in-time snapshot.
</aside>

<h4 id="elasticsearch">Elasticsearch</h4>

<p>Elasticsearch does not provide a backup utility; however node.js community members created <code class="prettyprint">elasticdump</code>.  In order to use <code class="prettyprint">elasticdump</code>, you must have Node.JS and NPM installed.  Binaries for installing Node and NPM are available at <a href="https://nodejs.org/download/release/latest/">https://nodejs.org/download/release/latest/</a>.</p>

<ol>
<li><a href="https://www.npmjs.com/package/elasticdump">Install <code class="prettyprint">elasticdump</code></a></li>
</ol>

<p><code class="prettyprint">npm install -g elasticdump</code><br>
2. <a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a><br>
3. On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</p>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
4. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get to the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
5. From the Compose service, select the target Elasticsearch deployment to access the Overview dashboard. Note the connection string  </p>

<p><img alt="Bluemix: Elasticsearch Overview" src="images/compose/backup/elastic/overview.png" /><br>
6. From the Overview page, select the <code class="prettyprint">Users</code> view and confirm that you have a valid and current user credentials.<br>
7. In order for the <code class="prettyprint">elasticdump</code> tool to create a backup of all indexes, the base URL must be specified as an <code class="prettyprint">--input</code> and a directory for <code class="prettyprint">--output</code>, as directed in the <a href="https://github.com/taskrabbit/elasticsearch-dump#multielasticdump">MultiElasticDump section of the documentation.</a></p>

<p><code class="prettyprint">elasticdump --input=https://admin:Passw0rd@sl-eu-lon-2-portal1.dblayer.com:10331/ --output=/tmp/</code></p>

<h4 id="redis">Redis</h4>

<aside class='notice'>
In order to use the `redis-cli`, you must install <a href="http://redis.io/download">Redis to your localhost</a>.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a><br></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.<br></li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get to the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target Redis deployment to access the Overview dashboard.</p>

<p><img alt="Bluemix: Redis Overview" src="images/compose/backup/redis/overview.png" /><br>
5. Scroll down on the Overview dashboard until you reach the <code class="prettyprint">Connection Info</code> section. Use these credentials with the <code class="prettyprint">redis-cli</code> tool from your local host.</p>

<p><img alt="Bluemix: Redis Credentials" src="images/compose/backup/redis/creds.png" /><br>
6. From your local host execute the following command to create a remote backup of Redis</p>

<p><code class="prettyprint">redis-cli -h &lt;CONNECTION&gt; -p 14250 -a &lt;PASSWORD&gt; --rdb dump.rdb</code></p>

<aside class='notice'>
Interested users can learn more at the <a href='http://redis.io/topics/rediscli#remote-backups-of-rdb-files'>documentation for <code>redis-cli</code></a>.
</aside>

<h4 id="rethinkdb">RethinkDB</h4>

<aside class='notice'>
RethinkDB, python, and the rethinkdb python client must be installed on your local host. For rethink installation instructions, see <a href='https://rethinkdb.com/docs/install/'>https://rethinkdb.com/docs/install/</a>.
<br>
The RethinkDB Python Client installation instructions are found at <a href='https://www.rethinkdb.com/docs/install-drivers/python/'>https://www.rethinkdb.com/docs/install-drivers/python/</a>.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get to the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target Rethink deployment to access the Overview dashboard.</p>

<p><img alt="Compose: Rethink Overview" src="images/compose/backup/rethinkdb/overview.png" />
5. From the Rethink Overview, scroll down to the <code class="prettyprint">Connection Info</code> section and click <code class="prettyprint">Show</code> and note the password. Then scroll down to the <code class="prettyprint">SSL Certificate (Self-Signed)</code> section.</p>

<p><img alt="Compose: Rethink SSL" src="images/compose/backup/rethinkdb/ssl.png" />
6. Copy the contents of the SSL certificate to a file (ie: <code class="prettyprint">compose.rethinkdb.crt</code>) on your local host</p>

<p><img alt="Compose: SSL Certificate" src="images/compose/backup/rethinkdb/cert.png" />
7. From your local host, execute the <code class="prettyprint">rethinkdb dump</code> command with the host, port, <code class="prettyprint">--tls-cert</code> and the <code class="prettyprint">-p</code> (password) flags:</p>

<p><code class="prettyprint">rethinkdb dump -p --tls-cert compose.rethinkdb.crt -c sl-us-dal-9-portal.1.dblayer.com:10828</code></p>

<p>Enter your password when prompted and a successful result will output something similar to the following:</p>

<p><code class="prettyprint">[========================================] 100%
9342 rows exported from 1 tables, with 0 secondary indexes
  Done (0.75 seconds)
Zipping export directory...
  Done (0 seconds)
</code></p>

<h4 id="from-bluemix">From Bluemix</h4>

<p>In theory, it is possible to be able to execute binary backup tools, such as <code class="prettyprint">mongodump</code> or <code class="prettyprint">pg_dump</code> from Bluemix. In practice, it can be a complex task, as it requires creating a custom buildpack to build the binary from source.  Additionally, the binary must be built with SSL/TLS support, since connecting to Compose requires an SSL certificate.  Due to the SSL requirement, the binary should be built with the openssl libraries from the Bluemix host, and not from any other host.</p>

<p>A backup from Bluemix may reduce the network latency from Compose in comparison to running backup commands from your local network; however, storing data on the filesystem of a Bluemix application is strongly discouraged and should be immediately stored in secure location, such as an object store.  Moreover, additional consideration will have to be given to coordination, if there are more than one application instances running of the backup process.</p>

<h4 id="multiple-database-backup">Multiple Database Backup</h4>

<p>The primary reason to do manual backups of a Compose database, is to coordinate a backup across multiple database in order to preserve a consistent state.  The application architecture determines the complexity of making consistent backups across databases. A common technique to ensure a consistent backup across a distributed system is to make data immutable and versioned.</p>

<h5 id="immutability-and-versioning">Immutability and Versioning</h5>
<pre class="highlight javascript"><code><span class="c1">// `crypto` provides the consistent hash</span>
<span class="kr">const</span> <span class="nx">crypto</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">'crypto'</span><span class="p">);</span>
<span class="kr">const</span> <span class="nx">hash</span> <span class="o">=</span> <span class="nx">crypto</span><span class="p">.</span><span class="nx">createHash</span><span class="p">(</span><span class="s1">'sha256'</span><span class="p">);</span>

<span class="c1">// `mongoose` provides an Object Model for MongoDB</span>
<span class="kd">var</span> <span class="nx">mongoose</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">'mongoose'</span><span class="p">)</span>
<span class="p">,</span> <span class="nx">Schema</span> <span class="o">=</span> <span class="nx">mongoose</span><span class="p">.</span><span class="nx">Schema</span>

<span class="c1">// Providing a model for version history of any object</span>
<span class="p">,</span> <span class="nx">versionModel</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">Schema</span><span class="p">({</span>
  <span class="na">version</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span>
  <span class="na">data</span><span class="p">:</span> <span class="nx">Schema</span><span class="p">.</span><span class="nx">Types</span><span class="p">.</span><span class="nx">Mixed</span>
<span class="p">})</span>
<span class="p">,</span> <span class="nx">Version</span> <span class="o">=</span> <span class="nx">mongoose</span><span class="p">.</span><span class="nx">model</span><span class="p">(</span><span class="s1">'Version'</span><span class="p">,</span> <span class="nx">versionModel</span><span class="p">)</span>

<span class="c1">// An example object model</span>
<span class="p">,</span> <span class="nx">userModel</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">Schema</span><span class="p">({</span>
    <span class="na">first_name</span><span class="p">:</span>  <span class="nb">String</span><span class="p">,</span>
    <span class="na">middle_name</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span>
    <span class="na">last_name</span><span class="p">:</span>   <span class="nb">String</span><span class="p">,</span>
    <span class="na">role</span><span class="p">:</span>        <span class="nb">String</span><span class="p">,</span>
    <span class="na">dob</span><span class="p">:</span>         <span class="nb">Date</span><span class="p">,</span>
    <span class="na">version</span><span class="p">:</span>     <span class="nb">String</span><span class="p">,</span>
    <span class="na">history</span><span class="p">:</span>     <span class="p">[</span><span class="nx">Version</span><span class="p">]</span>
  <span class="p">})</span>
<span class="p">,</span> <span class="nx">User</span> <span class="o">=</span> <span class="nx">mongoose</span><span class="p">.</span><span class="nx">model</span><span class="p">(</span><span class="s1">'User'</span><span class="p">,</span> <span class="nx">userModel</span><span class="p">);</span>

<span class="c1">// Connect to Mongo</span>
<span class="nx">mongoose</span><span class="p">.</span><span class="nx">connect</span><span class="p">(</span><span class="nx">process</span><span class="p">.</span><span class="nx">env</span><span class="p">.</span><span class="nx">MONGO_URI</span><span class="p">,</span> <span class="p">{</span>
  <span class="na">user</span><span class="p">:</span> <span class="nx">process</span><span class="p">.</span><span class="nx">env</span><span class="p">.</span><span class="nx">MONGO_USER</span><span class="p">,</span>
  <span class="na">pass</span><span class="p">:</span> <span class="nx">process</span><span class="p">.</span><span class="nx">env</span><span class="p">.</span><span class="nx">MONGO_PASS</span>
<span class="p">});</span>

<span class="c1">// an example document to populate the object model</span>
<span class="kd">var</span> <span class="nx">user</span> <span class="o">=</span> <span class="p">{</span>
  <span class="na">first_name</span><span class="p">:</span> <span class="s1">'Sidney'</span><span class="p">,</span>
  <span class="na">middle_name</span><span class="p">:</span> <span class="s1">'R.'</span><span class="p">,</span>
  <span class="na">last_name</span><span class="p">:</span> <span class="s1">'Garfield'</span><span class="p">,</span>
  <span class="na">role</span><span class="p">:</span> <span class="s1">'physician'</span><span class="p">,</span>
  <span class="na">dob</span><span class="p">:</span> <span class="k">new</span> <span class="nb">Date</span><span class="p">(</span><span class="mi">1906</span><span class="p">,</span> <span class="mi">04</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span>
<span class="p">}</span>
<span class="c1">// crypto requires a string or buffer (ie: `json`)</span>
<span class="p">,</span> <span class="nx">json</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">record</span><span class="p">)</span>
<span class="p">,</span> <span class="nx">doc</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">User</span><span class="p">(</span><span class="nx">user</span><span class="p">)</span>
<span class="p">,</span> <span class="nx">version</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">Version</span><span class="p">({</span>
  <span class="na">data</span><span class="p">:</span> <span class="nx">user</span><span class="p">,</span>
<span class="c1">// `hex` will marshall a string to fit `Version` schema</span>
  <span class="na">version</span><span class="p">:</span> <span class="nx">hash</span><span class="p">.</span><span class="nx">update</span><span class="p">(</span><span class="nx">json</span><span class="p">).</span><span class="nx">digest</span><span class="p">(</span><span class="s1">'hex'</span><span class="p">)</span>
<span class="p">});</span>

<span class="c1">// Saves the document with the version history and the latest version</span>
<span class="nx">doc</span><span class="p">.</span><span class="nx">version</span> <span class="o">=</span> <span class="nx">version</span><span class="p">.</span><span class="nx">version</span><span class="p">;</span>
<span class="nx">doc</span><span class="p">.</span><span class="nx">history</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">version</span><span class="p">);</span>
<span class="nx">doc</span><span class="p">.</span><span class="nx">save</span><span class="p">();</span>
</code></pre>
<pre class="highlight java"><code><span class="kn">package</span> <span class="n">com</span><span class="o">.</span><span class="na">ibm</span><span class="o">.</span><span class="na">immutability</span><span class="o">.</span><span class="na">example</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">com.mongodb.MongoClient</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mongodb.morphia.Datastore</span><span class="o">;</span>
<span class="c1">// `Morphia` provides an Object Model for MongoDB</span>
<span class="kn">import</span> <span class="nn">org.mongodb.morphia.Morphia</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mongodb.morphia.annotations.Entity</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mongodb.morphia.annotations.Field</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mongodb.morphia.annotations.Id</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mongodb.morphia.annotations.Index</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mongodb.morphia.annotations.Indexes</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mongodb.morphia.annotations.Property</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="o">;</span>
<span class="c1">// `java.security.MessageDigest` provides the consistent hash</span>
<span class="kn">import</span> <span class="nn">java.security.MessageDigest</span><span class="o">;</span>
<span class="c1">// JSON utils</span>
<span class="kn">import</span> <span class="nn">org.codehaus.jackson.JsonGenerationException</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.codehaus.jackson.map.JsonMappingException</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.codehaus.jackson.map.ObjectMapper</span><span class="o">;</span>



<span class="kd">public</span> <span class="kd">final</span> <span class="kd">class</span> <span class="nc">ImmutabilityDemo</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="n">ImmutabilityDemo</span><span class="o">(){</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="n">main</span><span class="o">(</span><span class="kd">final</span> <span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
    <span class="kd">final</span> <span class="n">Morphia</span> <span class="n">morphia</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Morphia</span><span class="o">();</span>

    <span class="c1">// tell morphia where to find your classes</span>
    <span class="c1">// can be called multiple times with different packages or classes</span>
    <span class="n">morphia</span><span class="o">.</span><span class="na">mapPackage</span><span class="o">(</span><span class="s">"org.ibm.immutability.example"</span><span class="o">);</span>

    <span class="c1">// create the Datastore connecting to the database running on the default port on the local host</span>
    <span class="kd">final</span> <span class="n">Datastore</span> <span class="n">datastore</span> <span class="o">=</span> <span class="n">morphia</span><span class="o">.</span><span class="na">createDatastore</span><span class="o">(</span><span class="k">new</span> <span class="n">MongoClient</span><span class="o">(),</span> <span class="s">"immutability_example"</span><span class="o">);</span>

    <span class="n">SimpleDateFormat</span> <span class="n">ft</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SimpleDateFormat</span> <span class="o">(</span><span class="s">"yyyy-MM-dd"</span><span class="o">);</span>
    <span class="n">Date</span> <span class="n">dob</span><span class="o">;</span>
    <span class="n">dob</span> <span class="o">=</span> <span class="n">ft</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="s">"1906-04-17"</span><span class="o">);</span>
    <span class="kd">final</span> <span class="n">User</span> <span class="n">user</span> <span class="o">=</span> <span class="k">new</span> <span class="n">User</span><span class="o">(</span><span class="s">"Sidney"</span><span class="o">,</span> <span class="s">"R."</span><span class="o">,</span> <span class="s">"Garfiled"</span><span class="o">,</span> <span class="s">"physician"</span><span class="o">,</span> <span class="n">dob</span><span class="o">);</span>
    <span class="c1">// an example document to populate the object model</span>

    <span class="n">datastore</span><span class="o">.</span><span class="na">save</span><span class="o">(</span><span class="n">user</span><span class="o">);</span>
  <span class="o">}</span>  
<span class="o">}</span>

<span class="c1">// Provides an example User model object</span>
<span class="nd">@Entity</span><span class="o">(</span><span class="s">"users"</span><span class="o">)</span>
<span class="kd">class</span> <span class="nc">User</span> <span class="o">{</span>
  <span class="nd">@Id</span>
  <span class="kd">private</span> <span class="n">ObjectId</span> <span class="n">id</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">firstName</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">middleName</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">lastName</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">role</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">Date</span> <span class="n">dob</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">version</span><span class="o">;</span>
  <span class="nd">@Reference</span>
  <span class="kd">private</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Version</span><span class="o">&gt;</span> <span class="n">history</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Version</span><span class="o">&gt;();</span>

  <span class="kd">public</span> <span class="n">User</span><span class="o">(</span>
      <span class="kd">final</span> <span class="n">String</span> <span class="n">firstName</span><span class="o">,</span>
      <span class="kd">final</span> <span class="n">String</span> <span class="n">middleName</span><span class="o">,</span>
      <span class="kd">final</span> <span class="n">String</span> <span class="n">lastName</span><span class="o">,</span>
      <span class="kd">final</span> <span class="n">String</span> <span class="n">role</span><span class="o">,</span>
      <span class="kd">final</span> <span class="n">Date</span> <span class="n">dob</span><span class="o">,</span>
    <span class="o">){</span>
    <span class="k">this</span><span class="o">.</span><span class="na">firstName</span> <span class="o">=</span> <span class="n">firstName</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">middleName</span> <span class="o">=</span> <span class="n">middleName</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">lastName</span> <span class="o">=</span> <span class="n">lastName</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">role</span> <span class="o">=</span> <span class="n">role</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">dob</span> <span class="o">=</span> <span class="n">dob</span><span class="o">;</span>

    <span class="kd">final</span> <span class="n">version</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Version</span><span class="o">(</span><span class="k">this</span><span class="o">);</span>
    <span class="k">this</span><span class="o">.</span><span class="na">version</span> <span class="o">=</span> <span class="n">version</span><span class="o">.</span><span class="na">version</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">history</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">version</span><span class="o">);</span>

    <span class="k">return</span> <span class="k">this</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// Provides a model for version history of a `User` object</span>
<span class="nd">@Entity</span><span class="o">(</span><span class="s">"versions"</span><span class="o">)</span>
<span class="kd">class</span> <span class="nc">Version</span> <span class="o">{</span>
  <span class="nd">@Id</span>
  <span class="kd">private</span> <span class="n">ObjectId</span> <span class="n">id</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">version</span><span class="o">;</span>
  <span class="nd">@Reference</span>
  <span class="kd">private</span> <span class="n">User</span> <span class="n">data</span><span class="o">;</span>

  <span class="kd">public</span> <span class="n">Version</span><span class="o">(</span><span class="kd">final</span> <span class="n">User</span> <span class="n">user</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">data</span> <span class="o">=</span> <span class="n">user</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">version</span> <span class="o">=</span> <span class="k">this</span><span class="o">.</span><span class="na">createVersionId</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="kd">private</span> <span class="n">String</span> <span class="n">createVersionId</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">ObjectMapper</span> <span class="n">mapper</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ObjectMapper</span><span class="o">();</span>
    <span class="c1">// Convert object to JSON string</span>
    <span class="n">String</span> <span class="n">json</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="na">writeValueAsString</span><span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="na">data</span><span class="o">);</span>

    <span class="n">MessageDigest</span> <span class="n">md</span> <span class="o">=</span> <span class="n">MessageDigest</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="s">"SHA-256"</span><span class="o">);</span>
    <span class="n">md</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="n">json</span><span class="o">.</span><span class="na">getBytes</span><span class="o">(</span><span class="s">"UTF-8"</span><span class="o">));</span>
    <span class="kt">byte</span><span class="o">[]</span> <span class="n">digest</span> <span class="o">=</span> <span class="n">md</span><span class="o">.</span><span class="na">digest</span><span class="o">();</span>
    <span class="n">String</span>  <span class="n">versionString</span> <span class="o">=</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">"%064x"</span><span class="o">,</span> <span class="k">new</span> <span class="n">java</span><span class="o">.</span><span class="na">math</span><span class="o">.</span><span class="na">BigInteger</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="n">digest</span><span class="o">));</span>

    <span class="k">return</span> <span class="n">versionString</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre>

<p>Immutability solves creating consistent backups, not by coordination, but by maintaining a version history of the state of a data object. While this optimizes for recoverability at the last known good state and high availability, the tradeoff is an additional use of disk space, as well as complexity created by maintaining a version history of a data object.</p>

<aside class='success'>
A common workaround the increased disk usage is to use a bounded history size.
</aside>

<p>It is <strong><a href="https://en.wikipedia.org/wiki/Clock_synchronization">near impossible</a></strong> to coordinate database backups according to wall clock time, because of the complexity required to coordinate and synchronize all CPU clocks across multiple servers. Therefore, if the application has access to an array of versions, then the versions of data can be coordinated in the application layer, rather than relying on precise timing to capture state at a precise moment.</p>

<aside class='warning'>
While immutability simplifies issues regarding managing state across multiple databases, it adds complexity to querying, updates, and operations.
</aside>

<p>A frequently used technique is to use a consistent hashing algorithm with a low collision rate (ie: <code class="prettyprint">SHA256</code>) to create a version identifier based on the contents of the data object. See the example on the right.</p>

<h5 id="halting-database-writes">Halting Database Writes</h5>

<p>If an application uses multiple databases and is not architected to use <a href="#immutability-and-versioning">versioning for data objects</a>, the best way to capture a consistent state across multiple databases is to halt writes to all databases.  This can be achieved by planning an outage for an application.  </p>

<p>An outage is a less-than-ideal solution if your application requires high-availability. To minimize the impact of halting writes to the databases, the application could be architected to take advantage of message queues (ie: MessageHub) and the <a href="https://www.redbooks.ibm.com/abstracts/sg248275.html">micro-services architectural pattern</a>. In this scenario, micro-services are created to accept writes for each database. These applications can be coordinated to stop accepting writes and take a snapshot of the database.</p>

<p>A message queue, such as MessageHub, could be used to fan data objects to the datastore micro-services. In this case, the queue can still accept the objects for writes, while the micro-services are halted for capturing consistent snapshots.</p>

<p>The addition of a caching micro-service, such as Redis, may allow your application to have a read-your-write consistency model for some services; however services requiring backup, such as an Elasticsearch full-text search service, would not have the latest dataset to query until writes to the service were restored.</p>

<h4 id="replication-and-failover">Replication and Failover</h4>

<p>Many of the open source databases vendors may provide replication products which are unsupported in Compose.  </p>

          <!--- CLIENT SPECIFIC DATA --->

<h2 id="restore">Restore</h2>

<p>Compose&rsquo;s user interface (UI) provides a method to to restore databases from  <a href="#on-demand"><em>On-Demand</em></a> or <a href="#scheduled"><em>Scheduled</em></a> backups. Restoring a database through Compose&rsquo;s UI is straight-forward, but does warrant a discussion with regards to best practices. Detailed below will are topics that should be understood before starting a backup and restore process.</p>

<h3 id="how-restore-works">How Restore Works</h3>

<p>To understand how to restore, it is important to understand <a href="#design">how backups are designed</a>. In short, for most deployments, backups are taken from a hidden cluster member, in which writes are halted and then the database files are archived from the filesystem into an object store.</p>

<p>In Compose, restoring from a backup creates a new deployment identical to the previous deployment. Compose will pull the data from the remote object store, place the archive files onto the filesystem, and then start the new database processes in the cluster. This restores the database to the point in time from when the backup was created.</p>

<p>Only <code class="prettyprint">etcd</code> and <code class="prettyprint">rethinkdb</code> are not restored with the original database files.  In these databases, <a href="https://github.com/mickep76/etcdtool"><code class="prettyprint">etcdtool</code></a> and <a href="https://rethinkdb.com/docs/backup/#restore"><code class="prettyprint">rethink restore</code></a> are used, respectively. This is an implementation detail and does not affect how users make use of Compose&rsquo;s restore functionality.</p>

<h3 id="compose-restore-instructions">Compose Restore Instructions</h3>

<p>The following instructions are how to restore a deployment for a Bluemix dedicated application, generalized for all types of Compose database deployments.  </p>

<aside class='warning'>
When using the Compose UI to restore a database, Compose will restore to a new deployment (see <a href='#how-restore-works'>How Restore Works</a>), which means using the default <code>VCAP_SERVICES</code> credentials <strong>may not be usable with the new deployment connection strings</strong>.  The application will need <a href='#user-provided-service-binding'>user-provided services</a>to access the new deployment connection string.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get through the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. Select the deployment to access the Overview page, for example a MongoDB deployment</p>

<p><img alt="Compose: MongoDB Overview" src="images/compose/backup/mongo/overview.png" /><br>
5. From the Overview dashboard, select the <code class="prettyprint">Backups</code> to view the archive of Scheduled and On-Demand backups.</p>

<p><img alt="Compose: MongoDB Backups" src="images/compose/backup/backups.png" /><br>
6. From the Backups view, there are two options, either download or restore.  In order to restore to a new deployment, click the Restore icon.</p>

<p><img alt="Compose: Restore" src="images/compose/restore/mongo/restore.png" /><br>
7. On the Restore view, deployment specific options (such as SSL or storage engines) will be displayed, as well as the target datacenter. Unless the location of the Bluemix application has changed, ensure the proper target datacenter is chosen.</p>

<aside class='warning'>
Restoring to the wrong target location, such as a public cloud, could be a liability or risk for sensitive data.  Be careful!
</aside>

<p><img alt="Compose: Restore" src="images/compose/restore/mongo/restore2.png" /><br>
8. Once the restore process is initiated, you will see the following notification. The time for the process to provision an new deployment is a function of the size of the previous database; however, because it is being restored from the database files from the filesystem, the restore process is generally quick.</p>

<p><img alt="Compose: Restore Deployment" src="images/compose/restore/mongo/provisioning.png" /><br>
9. Once the provisioning of the new deployment is complete, then go to the Overview dashboard and scroll to the connection information.  </p>

<aside class='warning'>
If the application uses <code>VCAP_SERVICES</code> authentication, then the application will need to be reconfigured to use <a href='#user-provided-service-binding>user-provided services</a>.
</aside>

<aside class='warning'>
When an application uses SSL authentication, such as MongoDB or RethinkDB, then the application will need to be reconfigured to use a new SSL certificate. If the SSL certificate is read from the filesystem, add the new certificate and redeploy the application.  Otherwise, applications can use <a href='#user-provided-service-binding'>user-provided service bindings</a> to provide the certificate (and other credentials).
</aside>

<p><img alt="Compose: Overview" src="images/compose/backup/mongo/overview.png" /></p>

<ol>
<li>With the new deployment connection string, update the <a href="#user-provided-service-binding">user-provided services for the application</a>.</li>
</ol>

<aside class='warning'>
When adding user-provided services, all application instances will require to be restaged.
</aside>

<h4 id="considerations">Considerations</h4>

<h5 id="authentication-and-connections">Authentication and Connections</h5>

<p>The most important consideration when restoring a database, is how the application creates the connection - either through <code class="prettyprint">VCAP_SERVICES</code> or user-defined environment variables. When a Compose service is created and bound to an application, many template applications make use of the default connection information in <code class="prettyprint">VCAP_SERVICES</code>.  This becomes problematic when using the Compose database restore functionality, because of <a href="#how-restore-works">how restore  works</a> by creating a new database deployment.</p>

<p>If the application makes use of default <code class="prettyprint">VCAP_SERVICES</code>, then it is the best practice is to change the application to use user-provided service binding.  See <a href="#user-provided-service-binding">User Provided Service Binding</a> for an example.</p>

<p><strong>Manually restoring a database should be avoided.</strong>  See <a href="#manual-restore-instructions">the discussion below</a> for more details.</p>

<h5 id="coordinating-restore-over-multiple-database-deployments">Coordinating Restore over Multiple Database Deployments</h5>

<p>The Compose Restore functionality can be thought of as a <em>&ldquo;duplicate deployment&rdquo;</em> functionality, as opposed to <em>&ldquo;restore&rdquo;</em> to a running database deployment.  Compose restores   Using the Compose Restore UI, it is not possible to restore multiple databases at once.  Additionally, the Compose API has been deprecated for use with any deployment.  It is only available for legacy deployments of a <em>&ldquo;Classic MongoDB&rdquo;</em> deployment.</p>

<h5 id="restoring-complete-compose-cluster-all-deployments">Restoring Complete Compose Cluster (all deployments)</h5>

<p>As an administrator, it is not possible to restore all Compose databases for a given Bluemix Dedicated Cloud. Database restores need to be executed on a per-application basis. This role and responsibility is the application owners in a Bluemix Dedicated Cloud. There is no UI or API available to cloud administrators to restore all databases.</p>

<aside class='success'>
Compose automatically <a href="#scheduled">schedules backups </a> for all databases.
</aside>

<p><strong>Manually restoring a database should be avoided.</strong>  See <a href="#manual-restore-instructions">the discussion below</a> for more details.</p>

<h5 id="failure">Failure</h5>

<p>If a restore fails, the new database deployment will provide notice and the connection strings will not be available on the Overview dashboard.  If a failure occurs, the Compose team will be notified, however it is advised to submit a support ticket.</p>

<h3 id="manual-restore-instructions">Manual Restore Instructions</h3>

<p>A manual database restoration is best suited for local development environments or a migration from a database deployment that is not in Compose.</p>

<aside class='warning'>
When doing a manual restore to any Compose deployment the network latency may be an issue, depending on the size of the database.
</aside>

<h4 id="mongodb">MongoDB</h4>

<aside class='notice'>
The MongoDB tools <code>mongorestore</code> or <code>mongodump</code> is required. For local development databases, MongoDB must also be installed.
</aside>

<h5 id="using-a-compose-mongodb-backup">Using A Compose MongoDB Backup</h5>

<p>Compose MongoDB backup files are not created with <code class="prettyprint">mongodump</code>, instead they snapshots of the database files.  See <a href="#design">Backup Design</a> for more information.</p>

<ol>
<li><a href="#accessibility">Download a backup from a MongoDB deployment from Compose</a></li>
<li>Decompress the tar archive</li>
<li>Start MongoDB locally with the database files:<br>
<code class="prettyprint">monogod --dbpath /path/to/directory</code></li>
</ol>

<h5 id="using-a-manual-backup">Using a Manual Backup</h5>

<ol>
<li><a href="#mongodb">Obtain a Manual backup</a></li>
<li><a href="https://docs.mongodb.com/manual/tutorial/backup-and-restore-tools/#basic-mongorestore-operations">Follow the <code class="prettyprint">mongorestore</code> instructions</a></li>
</ol>

<h6 id="for-a-compose-deployment">For a Compose deployment</h6>

<p>While <code class="prettyprint">mongorestore</code> is a well documented tool for Mongo database restoration, for production environments in Compose the best practice is to follow the <a href="#compose-restore-instructions">instructions for Compose Restore</a>.  If an application makes use of the <code class="prettyprint">VCAP_SERVICES</code> connection strings, it may also make sense to create a new Compose service binding to your application and then use this method to restore the database.</p>

<aside class='warning'>
These instructions could overwrite any data in an existing database and may or may experience network issues, depending the size of the database and the quality of the network route between client and server.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get through the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target MongoDB deployment Overview dashboard, and obtain the connection information.</p>

<p><img alt="Bluemix: Mongo Overview" src="images/compose/backup/mongo/overview.png" /><br>
5. Execute the following command to restore to Compose-hosted MongoDB deployment:</p>

<p><code class="prettyprint">mongorestore --ssl --sslAllowInvalidCertificates -h sl-us-dal-9-portal.1.dblayer.com -p 10233 -d DBNAME -u USERNAME -pPASSWORD</code></p>

<aside class='notice'>
If the target database already contains the collections that are part of the dataset that will be restored, consider adding the <code>--drop</code> flag to <code>mongorestore</code>
</aside>

<p>More about <code class="prettyprint">mongorestore</code> can be found on <a href="https://docs.mongodb.com/manual/reference/program/mongorestore/">the MongoDB documentation</a></p>

<h4 id="postgres">Postgres</h4>

<aside class='notice'>
The Postgres tools <code>psql</code> may be required. For local development databases, Postgres must also be installed.
</aside>

<h5 id="using-a-compose-postgres-backup">Using A Compose Postgres Backup</h5>

<p>Compose Postgres backup files are not created with <code class="prettyprint">pg_dump</code>, instead they <code class="prettyprint">pg_basebackup</code>, which produces a set of data files which can be used to start Postgres. See <a href="https://help.compose.com/docs/postgresql-faq#how-do-backups-work">the Compose documentation</a> and the <a href="https://www.postgresql.org/docs/9.1/static/app-pgbasebackup.html"><code class="prettyprint">pg_basebackup</code> documentation</a> for more information.</p>

<p>To install a Compose Backup on your local Postgres installation:  </p>

<ol>
<li><a href="#accessibility">Download a backup from a Postgres deployment from Compose</a></li>
<li>Decompress the tar archive</li>
<li>Follow <a href="https://www.postgresql.org/docs/9.1/static/continuous-archiving.html#BACKUP-PITR-RECOVERY">the instructions for a Point-In-Time-Recovery</a></li>
</ol>

<h5 id="using-a-manual-backup">Using a Manual Backup</h5>

<p>For a local Postgres installation:</p>

<aside class='notice'>
These instructions require having the <code>pg_restore</code> tool installed locally.
</aside>

<ol>
<li><a href="#postgresql">Obtain a Manual backup</a></li>
<li><a href="https://www.postgresql.org/docs/current/static/app-pgrestore.html">Follow the <code class="prettyprint">pg_restore</code> instructions</a></li>
</ol>

<h6 id="for-a-compose-deployment">For a Compose deployment</h6>

<p>While <code class="prettyprint">pg_restore</code> is a well documented tool for Postgres database restoration, for production environments in Compose the best practice is to follow the <a href="#compose-restore-instructions">instructions for Compose Restore</a>.  If an application makes use of the <code class="prettyprint">VCAP_SERVICES</code> connection strings, it may also make sense to create a new Compose service binding to your application and then use this method to restore the database.</p>

<aside class='warning'>
These instructions could overwrite any data in an existing database and may or may experience network issues, depending the size of the database and the quality of the network route between client and server.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get through the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target Postgres deployment Overview dashboard, and obtain the connection information.</p>

<p><img alt="Bluemix: Postgres Overview" src="images/compose/backup/postgres/overview.png" />
5.  Use <code class="prettyprint">pg_restore</code> with the connection strings provided by Compose. For example:</p>

<p><code class="prettyprint">pg_restore -d DBNAME -h &lt;HOST NAME&gt; -p 10109 -U admin /path/to/dump</code></p>

<p>More about <code class="prettyprint">pg_restore</code> can be found on <a href="https://www.postgresql.org/docs/current/static/app-pgrestore.html">the Postgres documentation</a></p>

<h4 id="elasticsearch">ElasticSearch</h4>

<p>Elasticsearch does not provide a tool for dumping and restoring an Elasticsearch database. However, an tool called <a href="https://www.npmjs.com/package/elasticdump"><code class="prettyprint">elasticdump</code></a> was provided by the open-source community. Its use requires that Node and NPM are installed.</p>

<p>Compose does provide the ability to download a backup an Elasticsearch cluster; however following the <a href="#compose-restore-instructions">Compose Restore Instructions</a>, it is possible to restore an Elasticsearch database in Compose.</p>

<h5 id="using-a-manual-backup">Using a Manual Backup</h5>

<h6 id="for-a-local-elasticsearch-installation">For a local Elasticsearch installation:</h6>

<aside class='notice'>
These instructions require having the <code>elasticdump</code> tool installed locally.
</aside>

<p>See the <a href="#elasticsearch">Elasticsearch manual backup section for instructions on how to install <code class="prettyprint">elasticdump</code></a></p>

<ol>
<li><a href="#elasticsearch">Obtain a Manual backup</a> - <em>NOTE: It is possible to read from one Compose Elasticsearch deployment and into a local or remote elasticsearch host.</em></li>
<li><a href="https://github.com/taskrabbit/elasticsearch-dump#standard-install">Follow the <code class="prettyprint">elasticdump</code> instructions to restore</a>.  For example:</li>
</ol>

<p><code class="prettyprint">elasticdump --input=path/to/elasticdump.json --output=https://USER:PASSWORD@&lt;HOSTNAME&gt;:10331 --type=data</code></p>

<h6 id="for-a-compose-deployment">For a Compose deployment</h6>

<p>For production environments in Compose, the best practice is to follow the <a href="#compose-restore-instructions">instructions for Compose Restore</a>.  </p>

<aside class='warning'>
These instructions could overwrite any data in an existing database and may or may experience network issues, depending the size of the database and the quality of the network route between client and server.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get through the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target Elasticsearch deployment Overview dashboard, and obtain the connection information.</p>

<p><img alt="Bluemix: Elasticsearch Overview" src="images/compose/backup/elastic/overview.png" />
5.  Use <code class="prettyprint">elasticdump</code> with the connection strings provided by Compose. For example:</p>

<p><code class="prettyprint">elasticdump --input=https://USER1:PASSWORD1@&lt;HOSTNAME&gt;:10331 --output=https://USER2:PASSWORD2@&lt;HOSTNAME&gt;:10332 --type=data</code></p>

<h4 id="redis">Redis</h4>

<aside class='notice'>
The Redis tool <code>redis-cli</code> or <code>mongodump</code> is required. For local development databases, MongoDB must also be installed.
</aside>

<h5 id="using-a-compose-redis-backup">Using A Compose Redis Backup</h5>

<p>Compose Redis backup files are created with <code class="prettyprint">redis-cli</code> and dump the rdb file. These instructions can be used to create a <code class="prettyprint">redis-server</code> for local development:</p>

<ol>
<li><a href="#accessibility">Download a backup from a Redis deployment from Compose</a></li>
<li>Decompress the tar archive</li>
<li>Start Redis locally with the database</li>
<li>Import the dump file:
&lt;!&ndash; TODO: Verify on Client Cloud&gt; &ndash;&gt;
<code class="prettyprint">redis-server --dbfilename dump.rdb --dir .</code></li>
</ol>

<aside class='notice'>
The <code>dbfilename</code> and <code>dir</code> can be set through the <code>redis-server</code> configuration file.
</aside>

<h5 id="using-a-manual-backup">Using a Manual Backup</h5>

<p>The instructions for a Manual backup are identical to the Compose provided backup file, <a href="#using-a-compose-redis-backup">mention directly above</a>.</p>

<h6 id="for-a-compose-deployment">For a Compose deployment</h6>

<p>Restoring a Compose Redis deployment for production environments in Compose should follow the best practice found in the <a href="#compose-restore-instructions">instructions for Compose Restore</a>.  If an application makes use of the <code class="prettyprint">VCAP_SERVICES</code> connection strings, it may also make sense to create a new Compose service binding to your application and then use this method to restore the database.</p>

<aside class='warning'>
These instructions could overwrite any data in an existing database and may or may experience network issues, depending the size of the database and the quality of the network route between client and server.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get through the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target MongoDB deployment Overview dashboard, and obtain the connection information.</p>

<p><img alt="Bluemix: Mongo Overview" src="images/compose/backup/redis/overview.png" />
5. Execute the following command to restore to Compose-hosted Redis deployment:</p>

<p><code class="prettyprint">redis-cli -h &lt;HOSTNAME&gt; -p 14250 -a SAQABRBLMAPXPXFM --rdb dump.rdb</code></p>

<aside class='notice'>
If the target database already contains the collections that are part of the dataset that will be restored, consider adding the <code>--drop</code> flag to <code>mongorestore</code>
</aside>

<p>More about <code class="prettyprint">redis-cli</code> can be found on <a href="http://redis.io/topics/rediscli">the Redis documentation</a></p>

<h4 id="rethinkdb">RethinkDB</h4>

<aside class='notice'>
The Rethink database and CLI tool <code>rethinkdb</code> is required for local development databases.
</aside>

<p>See the <a href="#rethinkdb">RethinkDB backup section for more information</a>.</p>

<p>All RethinkDB backup files are created with <code class="prettyprint">rethinkdb dump</code> into a file. These instructions can be used to recreate a RethinkDB for local development or for a Compose RethinkDB deploymnet.</p>

<ol>
<li><a href="#accessibility">Download a backup from a RethinkDB deployment from Compose</a></li>
<li>Decompress the tar archive</li>
<li>Start Rethink locally with the database</li>
<li>Import the dump file using <a href="https://rethinkdb.com/docs/backup/#restore"><code class="prettyprint">rethinkdb restore</code></a>
&lt;!&ndash; TODO: Verify on Client Cloud&gt; &ndash;&gt;
<code class="prettyprint">rethinkdb restore DATAFILE</code></li>
</ol>

<h5 id="for-a-compose-deployment">For a Compose Deployment</h5>

<aside class='warning'>
These instructions could overwrite any data in an existing database and may or may experience network issues, depending the size of the database and the quality of the network route between client and server.
</aside>

<ol>
<li><a href="https://console.kpsj001.us-west.bluemix.net/">Login to Bluemix</a></li>
<li>On the Bluemix left-hand navigation, click the <strong>Services</strong> link and a specific Compose Service deployment.</li>
</ol>

<p><img alt="Bluemix: Services" src="images/compose/services.png" /><br>
3. From the Compose service dashboard in Bluemix click the <strong>Launch</strong> button to get through the Compose dashboard.</p>

<p><img alt="Bluemix: Launch" src="images/compose/launch.png" /><br>
4. From the Compose service, select the target Rethink deployment Overview dashboard, and obtain the connection information.</p>

<p><img alt="Bluemix: RethinkDB Overview" src="images/compose/backup/rethinkdb/overview.png" />
5. Execute the following command to restore to Compose-hosted RethinkDB deployment:</p>

<p><code class="prettyprint">redis-rethinkdb restore rdb_dump.tar.gz --tls-cert compose.rethinkdb.crt -c &lt;HOSTNAME&gt;:39500</code></p>

<p>More about <code class="prettyprint">redis-cli</code> can be found on <a href="https://rethinkdb.com/docs/backup/#restore">the Redis documentation</a></p>

<h4 id="considerations">Considerations</h4>

<h5 id="from-bluemix">From Bluemix</h5>

<p>It is not advised to store data in a Bluemix application. Doing so will likely require custom buildpacks and require coordination.  The only advantage is reducing network latency, however the data would still be required to be pushed to the application instance.  In short, avoid if possible.</p>

<h5 id="performance">Performance</h5>

<p>The performance of a manual restore will vary greatly, depending on, but not limited to, database size, network latency, database type, etc. When possible, use Compose&rsquo;s restoration. It will be the fastest path to restoration.</p>

<aside class='warning'>
A manual restoration of a database may not be able to complete, due to networking constraints.
</aside>

<h5 id="failure">Failure</h5>

<p>For any restore, whether the deployment is local or on Compose, the best practice is to restore to a fresh and clean database.  Moreover, every effort should be made to restore from Compose, to make use of the best practices and stable and reliable tooling previously implemented.</p>

<p>If a failure is encounter while manually importing a database, please contact support.</p>

<aside class='warning'>
Due to myrid local environment configurations it may not be possible for support to resolve a restore issue. Use at your own risk.
</aside>

<h5 id="coordinating-restoring-multiple-deployments">Coordinating Restoring Multiple Deployments</h5>

<p>See <a href="#multiple-database-backup">the discussion in the backup section of the documentation about coordinating a manual backup and restore across multiple databases.</a></p>

<h5 id="restoring-complete-compose-cluster-all-deployments">Restoring Complete Compose Cluster (all deployments)</h5>

<p>As a system administrator, it is not possible to backup all database deployments in a Compose cluster, nor is it possible to restore all databases at once.  In a cloud environment, such as Bluemix Dedicated, the onus of restoring databases generally falls in the hands of the application owner.  </p>

          <h2 id="disaster-recovery-runbook">Disaster Recovery Runbook</h2>

<p>This documentation is to assist the IT administrators and application owners administrating applications on an IBM Bluemix Dedicated environment during a unplanned outage of the Compose Service.</p>

<aside class='warning'>
<strong>DISCLAIMER:</strong> Every outage is unique. This documentation is generalized to fit most scenarios and consideration must be given to each step in the context of the outage.
<br>
The time to complete each step below are only an estimation and will likely vary widely depending on the outage.
</aside>

<h3 id="roles-and-responsibilities">Roles and Responsibilities</h3>

<ul>
<li><strong>Client Application Owners</strong> - end-users that are the stakeholders with deployed applications on the Bluemix and Compose platforms. Application owners are responsible for configuring their applications with credentials and hostnames.</li>
<li><strong>Client IT Administrators</strong> - coordinates communication and incident management between the client and IBM&rsquo;s support teams. Client IT administrators may be required to adjust any egress network connectivity that may be required to reach the cluster(s) as they will have different IP addresses.</li>
<li><strong>Bluemix Support</strong> - provides communication to the client on current status, concerns and issues.</li>
<li><strong>Compose Support</strong> - provides incident response and management for the Compose service. Compose is responsible for standing up hosts, preparing backup data for restore, and loading data as necessary into new database instances deployed on the new cluster.</li>
<li><strong>Bluemix Operations</strong> - provides incident response and management for the Bluemix platform. Responsible for service broker deployment.</li>
<li><strong>Softlayer Operations</strong> - provides infrastructure support to Bluemix Support, Bluemix Operations, and Compose Support.</li>
</ul>

<h3 id="disaster-recovery-outline">Disaster Recovery Outline</h3>

<h4 id="1-incident">1. Incident</h4>

<p>The Compose service is no longer accessible from the Bluemix Environment. Application monitoring may begin to raise alerts to Client Application Owners and/or Client IT Administrators. Compose receives notifications from monitors about outage.</p>

<h4 id="2-submit-support-ticket">2. Submit Support Ticket</h4>

<p>Client IT Administrators submit a <strong>Severity 1</strong> support ticket with the Bluemix Dedicated Support team through the Bluemix administrative interface. The time to create and submit the support ticket will depend largely on the Client IT Administrators.</p>

<h4 id="3-support-response">3. Support Response</h4>

<p>The Bluemix Support team will initiate coordinating incident management and  response with Compose Support team and Client IT Administrators. The Bluemix Support team will make the best effort to response as soon as possible. The Service Level Objective (SLO) for Severity 1 response times is 1 hour.</p>

<h4 id="4-outage-identification">4. Outage Identification</h4>

<p>The Compose Support and Bluemix Operations team determine the Compose cluster is not fixable within the current data center.</p>

<h4 id="5-deployment-identification-confirmation">5. Deployment Identification/Confirmation</h4>

<blockquote>
<p>The Cloud Foundry CLI command to unbind services</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>cf unbind-service APP_NAME COMPOSE_SERVICE_INSTANCE
</code></pre>

<blockquote>
<p>Or the Bluemix CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>bx service unbind APP_NAME SERVICE_INSTANCE
</code></pre>

<p>The Compose Support team determines affected database deployments. The identified deployments are confirmed with the Client IT Administrators. The time objective is 10 minutes.</p>

<h4 id="6-unbind-compose-services-from-impacted-applications">6. Unbind Compose Services from Impacted Applications</h4>

<p>The Bluemix Support team would advise the Client IT Administrators to unbind the existing Compose services from applications. The Client IT Administrators should notify the Application Owners to unbind the Compose service from their respective applications. The time objective should be 10 minutes per application.</p>

<h4 id="7-disaster-recover-site-infrastructure-deployment">7. Disaster Recover Site Infrastructure Deployment</h4>

<p>The Compose Support requests that SoftLayer provisions the required server hardware at the new datacenter. The time objective is 1 hour.</p>

<h4 id="8-compose-services-deployment">8. Compose Services Deployment</h4>

<p>The Compose Support team initiates the deployment and configuration of the Compose services on the provisioned hardware. The time objective is 1 hour.</p>

<h4 id="9-network-configuration">9. Network Configuration</h4>

<p>The Compose Support team provides IP addresses and network configuration for the VPN to the Bluemix Operations team. The time objective is 10 minutes.</p>

<h4 id="10-service-broker-update">10. Service Broker Update</h4>

<p>The Bluemix Operations team configures a new service broker in the Bluemix Dedicated environment to the new Compose cluster deployed at the new datacenter. The time objective is 30 minutes.</p>

<h4 id="11-database-deployment-restore">11. Database Deployment Restore</h4>

<p>The Compose Support team initiates restore function for database deployments identified in the affected datacenter. Time variable per deployment instance. The recovery time objective for each database may take between 30 minutes and 2 hours to restore depending on data size, index size, and configuration. The recovery point objective is within the last 24 hours, based on the last available backup.</p>

<h4 id="12-service-broker-confirmation">12. Service Broker Confirmation</h4>

<p>The Compose Support and Bluemix Operations team verify the updated service broker is properly configured and functional. The time objective is 10 minutes.</p>

<h4 id="13-service-binding">13. Service Binding</h4>

<p>The Client Application Owners and/or Client IT Administrators create user-provided services in Bluemix for each application. <a href="#user-provided-service-binding">Please see the User Provide Service instructions</a>. The time objective is less than 10 minutes per application.</p>

<h4 id="14-application-recovery">14. Application Recovery</h4>

<blockquote>
<p>To restart an application using the Cloud Foundry CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>cf restage APP_NAME
</code></pre>

<blockquote>
<p>To restart an application using the Bluemix CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>bx app restage APP_NAME
</code></pre>

<blockquote>
<p>To redeploy an application using the Cloud Foundry CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="c"># Add updated Compose SSL certificate to filesystem</span>
<span class="gp">$ </span>cf push APP_NAME
</code></pre>

<blockquote>
<p>To redeploy an application using the Bluemix CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="c"># Add updated Compose SSL certificate to filesystem</span>
<span class="gp">$ </span>bx app push APP_NAME
</code></pre>

<p>The Client Application Owners should restart or redeploy the application based on the application needs.  The time objective is less than 10 minutes per application.</p>

<ul>
<li><strong>Filesystem</strong>: Applications that read the Compose SSL certificate from the filesystem will need to be redeployed with the updated Compose SSL certificate available from the Compose Dashboard.</li>
<li><strong>Services</strong>: Applications that read from environment variables, such as <code class="prettyprint">VCAP_SERVICES</code> or user-defined environment variables may only require the application to be restaged, in order for the application to re-read the new service credentials.</li>
</ul>

<aside class='warning'>
Pushing or restaging an application will require the build process to commence. Changes in buildpacks may cause unexpected issues.
</aside>

<h4 id="15-application-recovery-confirmation">15. Application Recovery Confirmation</h4>

<p>The Client Application Owners and Client IT Administrators verifies impacted applications are now able to respond to production requests. The time objective is less than 10 minutes per application.</p>

<h3 id="resolution">Resolution</h3>

<p>Migrating from the disaster recovery (DR) site would entail creating (or restoring) the original cluster to a known-good state, followed by a full backup of the DR site being taken and then applied on the original cluster.</p>

<p>The Compose and Bluemix Support teams are primarily responsible for coordinating the restoration and migration of the original datacenter site. The Compose and Bluemix support team will coordinate directly with the Client IT administrators.</p>

<h3 id="escalation-path">Escalation Path</h3>

<p>Client IT Administrators should contact their IBM account managers if an escalation is required.</p>

<h3 id="applicable-situations">Applicable Situations</h3>

<p>This runbook was designed to address a complete loss of the Compose service from the Bluemix platform. Applicable situations include, but are not limited to:</p>

<ul>
<li>Natural Disaster</li>
<li>Datacenter power loss</li>
<li>Irreparable loss of network connectivity</li>
</ul>

          <h2 id="user-provided-service-binding">User Provided Service Binding</h2>

<p>The examples below are for application owners and administrators. Each database in Compose has slightly different authentication scheme; however the schema of the JSON document for credentials in <code class="prettyprint">VCAP_SERVICES</code> is arbitrary. Ultimately, the key/value pairs in a user-provided service are for the user to define and only relevant to the application that consumes the <code class="prettyprint">VCAP_SERVICES</code> environment variables.</p>

<p>Below are examples for how to create and bind a user-provided service to an existing application. These directions can be used with any database deployment in Compose. Either the <a href="https://docs.cloudfoundry.org/cf-cli/install-go-cli.html">Cloud Foundry</a> or <a href="https://clis.ng.bluemix.net/ui/home.html">Bluemix</a> CLI is required, examples will be given for each.</p>

<h3 id="1-get-db-credentials">1. Get DB credentials</h3>

<p>Login to the Compose Dashboard, select the target MongoDB deployment, and on the Overview page, scroll down to the <code class="prettyprint">Connection Info</code> section and obtain the appropriate credentials and SSL certificate.</p>

<p><img alt="Compose: Overview" src="images/compose/backup/mongo/overview.png" /></p>

<h3 id="2-create-credentials-json">2. Create Credentials JSON</h3>

<blockquote>
<p><code class="prettyprint">credentials.json</code>  </p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
</span><span class="nt">"db_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mongodb"</span><span class="p">,</span><span class="w">
</span><span class="nt">"db_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"&lt;DB_NAME&gt;"</span><span class="p">,</span><span class="w">
</span><span class="nt">"db_user"</span><span class="p">:</span><span class="w"> </span><span class="s2">"&lt;DB_USER&gt;"</span><span class="p">,</span><span class="w">
</span><span class="nt">"db_pass"</span><span class="p">:</span><span class="w"> </span><span class="s2">"&lt;DB_PASS&gt;"</span><span class="p">,</span><span class="w">
</span><span class="nt">"ca_certificate_base64"</span><span class="p">:</span><span class="w"> </span><span class="s2">"[REDACTED]"</span><span class="p">,</span><span class="w">
</span><span class="nt">"uri"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mongodb://&lt;USER&gt;:&lt;PASSWORD&gt;@bluemix-sandbox-dal-9-portal.1.dblayer.com:15460/&lt;DB_NAME&gt;?ssl=true"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>Create a valid JSON document with the appropriate key/value pairs for the credentials, based on how the application is configured to consume <code class="prettyprint">VCAP_SERVICES</code>. Save the json document to your local filesystem.</p>

<h3 id="3-create-service">3. Create Service</h3>

<blockquote>
<p>Create a user-provided service with the Cloud Foundry CLI:</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>cf cups my-custom-service -p credentials.json
</code></pre>

<blockquote>
<p>Create a user-provided service with the Bluemix CLI:</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>bx service user-provided-create my-custom-service -p credentials.json
</code></pre>

<p>Using the <code class="prettyprint">cf</code> or <code class="prettyprint">bx</code> tool, create a new user-provided service.</p>

<h3 id="4-bind-service">4. Bind Service</h3>

<blockquote>
<p>Bind a user-provided service with the Cloud Foundry CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>cf bs my-app my-custom-service
</code></pre>

<blockquote>
<p>Bind a user-provided service with the Bluemix CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>bx service <span class="nb">bind </span>my-app my-custom-service
</code></pre>

<p>Bind the newly created user-provided service to the application</p>

<h3 id="5-restage">5. Restage</h3>

<blockquote>
<p>Restage the application with the Cloud Foundry CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>cf restage my-app
</code></pre>

<blockquote>
<p>Restage the application with the Bluemix CLI</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>bx app restage my-app
</code></pre>

<p>Finally, simply <code class="prettyprint">restage</code> the application</p>

          <h2 id="rto-and-rpo">RTO and RPO</h2>

<p>The following documents contain information about RTO and RPO for the Compose services:</p>

<table><thead>
<tr>
<th>Compose Service</th>
<th>URL</th>
</tr>
</thead><tbody>
<tr>
<td>MongoDB</td>
<td><a href="https://ibm.box.com/v/mongo-rto">MongoDB RTO</a></td>
</tr>
<tr>
<td>Postgres</td>
<td><a href="https://ibm.box.com/v/postgres-rto">Postgres RTO</a></td>
</tr>
<tr>
<td>Elastic Search</td>
<td><a href="https://ibm.box.com/v/elasticsearch-rto">Elastic Serch RTO</a></td>
</tr>
<tr>
<td>Redis</td>
<td><a href="https://ibm.box.com/v/redis-rto">Redis RTO</a></td>
</tr>
</tbody></table>

          <h1 id="cloudant">Cloudant</h1>

<p>Cloudant is not a traditional database, but rather a Database-as-a-Service (DBaaS). Moreover, Cloudant is not a relational database (aka RDBMS), but rather is a &ldquo;NoSQL datastore&rdquo;. As a NoSQL DBaaS, Cloudant is a new paradigm when compared to a RDBMS, does not provide the ability to do backups.</p>

<aside class='success'>
Cloudant has a beta release available, which enables Enterprise customers to create incremental backups. This feature is not available by default, and must be enabled. Read more at the <a href='https://docs.cloudant.com/backup-guide.html'>Cloudant&rsquo;s documentation site</a>.
</aside>

<p>Cloudant&rsquo;s design and architecture optimize for durability and availability of data, large datasets, and operational simplicity.  With regards to &ldquo;operational simplicity&rdquo;, Cloudant is designed to be resilient when facing server outages. Moreover, Cloudant makes use of three properties which insure the durability and availability of data:  </p>

<ul>
<li><a href="#internal-replication"><strong>Internal replication</strong></a>: data records are replicated to three different machines, ensuring the data is secure and available</li>
<li><a href="#multi-version-concurrency-control"><strong>Multi-Version Concurrency Control</strong></a>: the causality of data mutation (ie: <code class="prettyprint">UPDATE</code>) is deterministic and version conflicts are retained, and presented to the end user for resolution.</li>
<li><a href="#delete-tombstones"><strong>Delete Tombstones</strong></a>: <code class="prettyprint">DELETE</code> operations on a data document create an incremented version of the data, with no data in the current version.</li>
</ul>

<p>The above are properties of an <a href="#eventual-consistency"><strong>Eventually Consistent</strong> database</a>. With an eventually consistent design, traditional backups are not necessary because of the internal replication. In an eventually consistent system, like Cloudant, there are three backups of the data at all times.</p>

<p>Eventual consistency does provide better guarantees of data durability and availability in a single datacenter than a RDBMS. With that said, in the event of failure of the whole datacenter, the best practice advises to use <a href="#backup-and-replication">replication for backups and disaster recovery</a>.</p>

          <h2 id="eventual-consistency">Eventual Consistency</h2>

<p>At the core of Cloudant is an open-source database called <a href="https://couchdb.apache.org/">CouchDB</a>. Cloudant has extended CouchDB by adding many features, ranging from geospatial indexing to full-text search. Moreover, Cloudant made CouchDB an <a href="https://docs.cloudant.com/cap_theorem.html">eventually consistent</a> database service.</p>

<p>Eventual Consistency describes the <a href="https://en.wikipedia.org/wiki/Consistency_model#Eventual_consistency">consistency model</a> for data or state in a distributed system. In Cloudant&rsquo;s case, when the database service is provisioned, a cluster of servers (a.k.a.: <em>&ldquo;nodes&rdquo;</em>) are tasked to store data and respond to read/write requests. Cloudant is a distributed system.</p>

<p>In a distributed system, a user wants three guarantees of the data - <strong>consistency</strong>, <strong>availability</strong>, and <strong>partition tolerance</strong>. Partition tolerance refers to the ability of the system to continue to operate, despite network or host failure. <em>Consistency</em> implies that every host responding to a query for data will provide the identical response. <em>Availability</em> means the data is there when it is requested. However in failure states or concurrent operations, in a distributed system, the user must choose between consistency or availability of the data.</p>

<p>Eventual consistency is a stark difference to traditional RDBMS. RDBMSs&rsquo; are generally configured to have <em>master-slave</em> replication. If the master node is unavailable, then writes are unavailable and consistency is preserved. In Cloudant, if a node fails, any other node can accept a write, thus the data  remains available for write operations.</p>

<p>In failure cases or concurrent operations, Cloudant is optimized for availability, whereas traditional RDBMSs are optimized for consistency. Cloudant&rsquo;s internal replication is near real-time (bounded by network latency), so inconsistency is an exception, not the rule. Inconsistencies arise during node failures in a cluster, or during race conditions (aka concurrent operations).  Inconsistencies are resolved by the use of Cloudant&rsquo;s <a href="#multi-version-concurrency-control">Multi-Version Concurrency Control</a>. Moreover, as a Database-as-a-Service, node failure in a Cloudant cluster is not exposed to the end-users of Cloudant.</p>

<h3 id="internal-replication">Internal Replication</h3>

<p><a href="#eventual-consistency">Eventual consistency</a> implies that the database system maintains multiple copies of data. When data is sent to a Cloudant cluster, three (3) nodes in the cluster write the data to disk. Creating three copies of the data ensures the data remains highly available, even when one or more nodes in the cluster are unavailable.</p>

<p>When compared to a traditional RDBMS which may only store one copy of the latest version data, Cloudant stores 3 copies of the most recent versions of the data. By storing 3 versioned copies, this is analogous of maintaining two real-time backups of a traditional RDBMS.</p>

<p>A master-slave configuration of an RDBMS may achieve a similar assurance of having numerous copies of the data; however, it is ultimately limited in scale. Cloudant can scale data partitioning or operation effortlessly, whereas an RDBMS would become bounded or complex.</p>

<h3 id="multi-version-concurrency-control">Multi-Version Concurrency Control</h3>

<blockquote>
<p>first revision of the document</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"74b2be56045bed0c8c9d24b939000dbe"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"_rev"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1-7438df87b632b312c53a08361a7c3299"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Samsung Galaxy S4"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">""</span><span class="p">,</span><span class="w">
  </span><span class="nt">"price"</span><span class="p">:</span><span class="w"> </span><span class="mi">650</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<blockquote>
<p>second revision of the document</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"74b2be56045bed0c8c9d24b939000dbe"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"_rev"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2-61ae00e029d4f5edd2981841243ded13"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Samsung Galaxy S4"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Latest smartphone from Samsung"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"price"</span><span class="p">:</span><span class="w"> </span><span class="mi">650</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<blockquote>
<p>also second revision, conflicts with the previous one</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"74b2be56045bed0c8c9d24b939000dbe"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"_rev"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2-f796915a291b37254f6df8f6f3389121"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Samsung Galaxy S4"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">""</span><span class="p">,</span><span class="w">
  </span><span class="nt">"price"</span><span class="p">:</span><span class="w"> </span><span class="mi">600</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>The <a href="#internal-replication">internal replication</a> uses a versioning tracking system called <a href="https://docs.cloudant.com/mvcc.html"><em>&ldquo;Multi-Version Concurrency Control&rdquo;</em></a>(aka <em>MVCC</em>) to track the changes in the data. This means that when a record is updated/mutated, the previous data is not removed until two conditions are met:</p>

<ol>
<li>All nodes hosting the data agree about the latest version, there are no conflicts.</li>
<li>The user configurable compaction process identifies old, stale versions of the data to be removed.</li>
</ol>

<p>In a traditional RDBMS, an <code class="prettyprint">UPDATE</code> operation mutates the data record and the previous values are discarded.  In Cloudant, the previous values are stored for a period of time, until a <a href="https://docs.cloudant.com/mvcc.html#revisions"><em>compaction process</em></a> occurs.</p>

<p>MVCC assigning a version number and an identifier of the node which incremented version number. This makes the causality of the data mutation is deterministic. When failures or concurrent write operations occur, then the multiple versions in conflict are presented to the application layer (or end-user) for <a href="https://docs.cloudant.com/mvcc.html#distributed-databases-and-conflicts">resolution</a>.  </p>

<aside class='notice'>
An example can be seen to the right of the versions of a document.  More information about MVCC can be found on in <a href='https://docs.cloudant.com/mvcc.html'>Cloudant documentation</a>.
</aside>

<p>By versioning data, the potential for data loss is mitigated, because Cloudant ensures the latest data has been replicated to three other hosts. If there is a conflict from a failure or concurrent mutation, no data is lost, and the choice is given to the end-user on how to <a href="https://docs.cloudant.com/mvcc.html#how-to-resolve-conflicts">resolve conflicts</a>.  </p>

<h3 id="delete-tombstones">Delete Tombstones</h3>

<p>Data in Cloudant is not physically removed until a time and manner determined by an administrator, not an end-user or application. From the perspective of an application, when a <code class="prettyprint">DELETE</code> operation is executed, the data document is removed. From the internal Cloudant perspective, the data record&rsquo;s version is incremented per <a href="#multi-version-concurrency-control">MVCC</a> and marked as deleted. This is called a <a href="https://docs.cloudant.com/document.html#'tombstone'-documents">Tombstone Document</a>.</p>

<p>Tombstone documents are not deleted. The previous versions of the document will remain in the database until the compaction process occurs.  The tombstone document is ultimately used for replication purposes, to ensure the deleted version of the document is circulated to the other hosts of the data.</p>

<p>Tombstones documents make use of <a href="#multi-version-concurrency-control">MVCC</a>. Unlike RDBMS, data is deleted immediately and unrecoverable once deleted, unless restoring from a backup. Cloudant provides a window in which data is immediately recoverable, based on the compaction schedule.</p>

<p>The timeframe until the compaction process occurs is based on the throughput of the system.  Compaction can be adjusted in priority for Cloudant dedicated customers.  To adjust the compaction schedule, users with the <code class="prettyprint">cluster admin</code> role must create a support request with the Cloudant support team.</p>

<blockquote>
<p>An example of a filter function to remove tombstone documents from a replication stream:</p>
</blockquote>

<aside class='success'>
To preserve deleted documents, configure replication with a <a href="https://docs.cloudant.com/document.html#'tombstone'-documents">filter function</a> to remove tombstone documents from replication to a backup database.
</aside>

          <h2 id="backup-and-replication">Backup and Replication</h2>

<aside class='notice'>
The following documentation has been extended from its original form at the <a href='https://docs.cloudant.com/backup-guide-using-replication.html'>IBM Cloudant documentation site</a>
</aside>

<p>Cloudant&rsquo;s replication provides the facility to create a database backup,
and store it on a Cloudant cluster. From a replicated cluster, it is possible to restore a specific data document or an entire database.</p>

<p>Cloudant has been designed (<a href="#eventual-consistency">eventually consistent, highly-available</a>) so that backups for data redundancy is no longer necessary. Backups are still a good practice, but it is important to understand the purpose of the backup, in order to choose a backup strategy.</p>

<p>For example, if an application requires low latency, replication can be used multi-master configuration.  If low latency is not a concern, but application data must be highly available, then replication can be used for a warm-standby configuration.  If an application needs to meet regulatory requirements, replication can be used for point-in-time or incremental backups; however if a history of data documents needs to be stored for compliance purposes, it may be advantageous to use architect/design the application to store versions of documents.</p>

<p>In summary, it is important to consider the purpose of the backup in order to choose the strategy that will best meet the objective of the backup. Due to Cloudant&rsquo;s fundamental design choices, it is <strong>highly unlikely</strong> that a backup will be needed to recover an entire cluster. It is highly likely that the use case of your application will determine the backup strategy required.</p>

<h3 id="full-backups-and-point-in-time">Full Backups and Point-in-Time</h3>

<p>Full and point-in-time backups are trivial to create with replication. The method is to use Cloudant replication from the primary cluster to a secondary cluster. Once replication is complete, disable replication, note the point-in-time the database was created.</p>

<p>To create multiple point-in-time backups on a schedule, such as seven daily and four weekly, this would require significant disk space, especially if the dataset is large.</p>

<aside class='success'>
Replication can be initiated through Cloudant&rsquo;s user interface.  More information can be found at <a href='https://docs.cloudant.com/replication_guide.html#how-do-i-initiate-replication-via-the-dashboard'> Cloudant&rsquo;s documentation</a>.
</aside>

<h3 id="incremental-backups">Incremental backups</h3>

<aside class='success'>
Cloudant has a beta release available, which enables Enterprise customers to create incremental backups. This feature is not available by default, and must be enabled. Read more at the <a href='https://docs.cloudant.com/backup-guide.html'>Cloudant&rsquo;s documentation site</a>.
</aside>

<p>To create an incremental backup, a high-level description of the process is as follows:</p>

<ol>
<li>Perform an initial backup of the entire database by using <a href="https://docs.cloudant.com/replication_guide.html">replication</a>.</li>
<li>Schedule a job locally to backup the differences, or deltas, from the previous 24 hour period.</li>
</ol>

<aside class="warning">
You can configure a backup to trigger at regular intervals. However, each interval must be 24 hours or more.
In other words, you can run daily backups but not hourly backups.
</aside>

<h3 id="creating-an-incremental-backup">Creating an incremental backup</h3>

<p>Incremental backups save only the differences or &lsquo;deltas&rsquo; between backups.
Every 24 hours, the source database is replicated to a target database.</p>

<p>Replication uses sequence values to identify the documents changed during the 24-hour period. The backup operation works by using replication to get and store a <a href="https://docs.cloudant.com/replication_guide.html#checkpoints">checkpoint</a>.</p>

<p>The backup operation creates the name from a combination of the date and the backup task name. This name makes it easier to identify checkpoints during the recovery or roll up process.</p>

<p>To create an incremental backup, perform the following steps:
&lt;!&ndash; TODO: Verify on Client Cluster &ndash;&gt;</p>

<ol>
<li> Find the ID of the checkpoint document for the last replication. It is stored in the  <code class="prettyprint">_replication_id</code> field of the replication document, found in the <code class="prettyprint">_replicator</code> database.</li>
<li> Open the checkpoint document at <code class="prettyprint">/&lt;database&gt;/_local/&lt;_replication_id&gt;</code>, where <code class="prettyprint">&lt;_replication_id&gt;</code> is the ID you found in the previous step, and <code class="prettyprint">&lt;database&gt;</code> is the name of the source or the target database. The document usually exists on both databases but might only exist on one.</li>
<li> Search for the <code class="prettyprint">recorded_seq</code> field of the first element in the history array found in the checkpoint document.</li>
<li> Start replicating to the new incremental backup database, setting the <code class="prettyprint">since_seq</code> field in the replication document to the value of the <code class="prettyprint">recorded_seq</code> field found in the previous step.</li>
</ol>

<h3 id="restoring-a-database">Restoring a database</h3>

<p>To restore a database from incremental backups, replicate each incremental backup to a new database, starting with the most recent increment. While it may be intuitive to start restoration from the oldest backup, this is not advised. Starting replication from the newest/latest incremental backup first is faster because updated documents are only written to the target database once. Any documents older than a copy already present in the new database are skipped.</p>

<h3 id="an-example">An example</h3>

<p>This example shows how to:</p>

<ul>
<li>  Setup databases to use incremental backup.</li>
<li>  Run a full backup.</li>
<li>  Set up and run an incremental backup.</li>
<li>  Restore a backup.</li>
</ul>

<blockquote>
<p>Constants used in this guide</p>
</blockquote>
<pre class="highlight plaintext"><code># save base URL and the content type in shell variables
$ url='https://&lt;username&gt;:&lt;password&gt;@&lt;username&gt;.cloudant.com'
$ ct='Content-Type: application-json'
</code></pre>

<p>In this example, the following assumptions are made:</p>

<ol>
<li> Only one backup is made for only one database;</li>
<li>A full backup will be created on <em>Monday</em></li>
<li>An incremental backup will be created on <em>Tuesday</em></li>
</ol>

<p><code class="prettyprint">curl</code> and <a href="http://stedolan.github.io/jq/"><code class="prettyprint">jq</code></a> commands to run these operations.  Other excellent tools are <a href="https://github.com/eliangcs/http-prompt"><code class="prettyprint">http-prompt</code></a> and <a href="https://httpie.org/">HTTPie</a>.</p>

<h4 id="step-1-check-you-have-three-databases">Step 1: Check you have three databases</h4>

<blockquote>
<p>Check you have three databases to use with this example</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/original"</span>
<span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/backup-monday"</span>
<span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/backup-tuesday"</span>
</code></pre>

<p>For this example, three databases are required:</p>

<ul>
<li>  The original database, holding the canonical data to backup.</li>
<li>  Two incremental databases, for Monday (<code class="prettyprint">backup-monday</code>) and Tuesday (<code class="prettyprint">backup-tuesday</code>).</li>
</ul>

<h4 id="step-2-create-the-_replicator-database">Step 2: Create the <code class="prettyprint">_replicator</code> database</h4>

<blockquote>
<p>Create the <code class="prettyprint">_replicator</code> database</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/_replicator"</span>
</code></pre>

<p>If it does not exist, create the <code class="prettyprint">_replicator</code> database.</p>

<h4 id="step-3-back-up-the-entire-original-database">Step 3: Back up the entire (original) database</h4>

<blockquote>
<p>Run a full backup on Monday</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/_replicator/full-backup-monday"</span> -H <span class="s2">"</span><span class="nv">$ct</span><span class="s2">"</span> -d @backup-monday.json
</code></pre>

<blockquote>
<p><code class="prettyprint">backup-monday.json</code> must contain the following:</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"full-backup-monday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/original"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"target"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/backup-monday"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>On Monday, back up all data for the first time by using replication from the source <code class="prettyprint">original</code> to target <code class="prettyprint">backup-monday</code>.</p>

<h4 id="step-4-get-checkpoint-id">Step 4: Get checkpoint ID</h4>

<blockquote>
<p>Get checkpoint ID to help find the <code class="prettyprint">recorded_seq</code> value:</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span><span class="nv">replication_id</span><span class="o">=</span><span class="k">$(</span>curl <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/_replicator/backup-monday"</span> | jq -r <span class="s1">'._replication_id'</span><span class="k">)</span>
</code></pre>

<p>On Tuesday do an incremental backup, rather than another full backup.</p>

<p>To start the incremental backup, two values are needed:</p>

<ul>
<li>  The checkpoint ID.</li>
<li>  The <code class="prettyprint">recorded_seq</code> value.</li>
</ul>

<p>These values identify where the last backup ended, and determine where to start the next incremental backup. With these values it is possible to run the incremental backup.</p>

<p>Obtain the checkpoint ID value, which is stored in the <code class="prettyprint">_replication_id</code> field of the replication document in the <code class="prettyprint">_replicator</code> database.</p>

<h4 id="step-5-get-recorded_seq-value">Step 5: Get <code class="prettyprint">recorded_seq</code> value</h4>

<blockquote>
<p>Get <code class="prettyprint">recorded_seq</code> from original database</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span><span class="nv">recorded_seq</span><span class="o">=</span><span class="k">$(</span>curl <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/original/_local/</span><span class="k">${</span><span class="nv">replication_id</span><span class="k">}</span><span class="s2">"</span> | jq -r <span class="s1">'.history[0].recorded_seq'</span><span class="k">)</span>
</code></pre>

<p>After obtaining the checkpoint ID, use the ID to get the <code class="prettyprint">recorded_seq</code> value. This is found in the first element of the history array in the <code class="prettyprint">/_local/${replication_id}</code> document, within the original database.</p>

<p>The last document replicated from the original database can be determined by  the <code class="prettyprint">recorded_seq</code> value.</p>

<h4 id="step-6-run-an-incremental-backup">Step 6: Run an incremental backup</h4>

<blockquote>
<p>Start Tuesday&rsquo;s incremental backup</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/_replicator/incr-backup-tuesday"</span> -H <span class="s2">"</span><span class="k">${</span><span class="nv">ct</span><span class="k">}</span><span class="s2">"</span> -d @backup-tuesday.json
</code></pre>

<blockquote>
<p>where backup-tuesday.json contains the following:</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"incr-backup-tuesday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/original"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"target"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/backup-tuesday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"since_seq"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${recorded_seq}"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>With the checkpoint ID and <code class="prettyprint">recorded_seq</code>, the Tuesday&rsquo;s incremental backup can be initiated. This replicates all the document changes made <em>since</em> the last replication.</p>

<p>Once the replication finishes, an incremental backup will be complete. The backup consists of all the documents in the original database, and may be restored by retrieving the content of both the <code class="prettyprint">backup-monday</code> <em>and</em> <code class="prettyprint">backup-tuesday</code> databases.</p>

<h4 id="step-7-restore-the-monday-backup">Step 7: Restore the Monday backup</h4>

<blockquote>
<p>Restore from the <code class="prettyprint">backup-monday</code> database</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/_replicator/restore-monday"</span> -H <span class="s2">"</span><span class="nv">$ct</span><span class="s2">"</span> -d @restore-monday.json
<span class="c"># where restore-monday.json contains the following:</span>
</code></pre>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"restore-monday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/backup-monday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"target"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/restore"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"create-target"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>To restore from a backup, replicate the initial full backup, and any incremental backups to a new database.</p>

<p>For example, to restore Monday&rsquo;s state, replicate from the <code class="prettyprint">backup-monday</code> database.</p>

<h4 id="step-8-restore-the-tuesday-backup">Step 8: Restore the Tuesday backup</h4>

<blockquote>
<p>Restore Tuesday&rsquo;s backup to get the latest changes first</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/_replicator/restore-tuesday"</span> -H <span class="s2">"</span><span class="nv">$ct</span><span class="s2">"</span> -d @restore-tuesday.json
</code></pre>

<blockquote>
<p><code class="prettyprint">restore-tuesday.json</code> contains the following json document:</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"restore-tuesday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/backup-tuesday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"target"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/restore"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"create-target"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  
</span><span class="p">}</span><span class="w">
</span></code></pre>

<blockquote>
<p>Finish by restoring Monday&rsquo;s backup last</p>
</blockquote>
<pre class="highlight shell"><code><span class="gp">$ </span>curl -X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/_replicator/restore-monday"</span> -H <span class="s2">"</span><span class="nv">$ct</span><span class="s2">"</span> -d @restore-monday.json
</code></pre>

<blockquote>
<p><code class="prettyprint">restore-monday.json</code> contains the following json document:</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"restore-monday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/backup-monday"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"target"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${url}/restore"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>To restore Tuesday&rsquo;s database, first replicate from <code class="prettyprint">backup-tuesday</code> and then from <code class="prettyprint">backup-monday</code>.</p>

<p>It is possible to restore in chronological sequence; however, by using the reverse order, documents updated on Tuesday only need to be written to the target database once. Older versions of the document stored in the Monday database are ignored.</p>

<h3 id="best-practices">Best practices</h3>

<p>While the previous information outlines the basic incremental backup process,
each application has its own requirements and strategies for backups.
Here are a few best practices you might want to keep in mind.</p>

<h4 id="scheduling-backups">Scheduling backups</h4>

<p>Replication jobs can significantly increase the load on a cluster. If  backing up several databases, it is best to stagger the replication jobs for different times, or to a time when the cluster is less busy.</p>

<h5 id="changing-the-io-priority-of-a-backup">Changing the IO priority of a backup</h5>

<blockquote>
<p>Setting the IO priority</p>
</blockquote>
<pre class="highlight shell"><code>X PUT <span class="s2">"</span><span class="k">${</span><span class="nv">url</span><span class="k">}</span><span class="s2">/_replicator/io_priority"</span> -H <span class="s2">"</span><span class="nv">$ct</span><span class="s2">"</span> -d @io_priority.json
</code></pre>

<blockquote>
<p><code class="prettyprint">io_priority.json</code> contains the following json document:</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"source"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nt">"url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://user:pass@example.com/db"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"headers"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nt">"x-cloudant-io-priority"</span><span class="p">:</span><span class="w"> </span><span class="s2">"low"</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nt">"target"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nt">"url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://user:pass@example.net/db"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"headers"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nt">"x-cloudant-io-priority"</span><span class="p">:</span><span class="w"> </span><span class="s2">"low"</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<blockquote>
<p>The response body will look like:</p>
</blockquote>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"ok"</span><span class="p">:</span><span class="kc">true</span><span class="p">,</span><span class="w">
  </span><span class="nt">"id"</span><span class="p">:</span><span class="s2">"io_priority"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"rev"</span><span class="p">:</span><span class="s2">"1-c3bc51284a2cc0ea0167178d23a8f553"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>You can change the priority of backup jobs by adjusting the value of the <code class="prettyprint">x-cloudant-io-priority</code> field within the replication document.</p>

<ol>
<li> In the source and target fields, change the <code class="prettyprint">headers</code> object.</li>
<li> In the headers object, change the <code class="prettyprint">x-cloudant-io-priority</code> field value to <code class="prettyprint">&quot;low&quot;</code>.</li>
</ol>

<h4 id="backing-up-design-documents">Backing up design documents</h4>

<blockquote>
<p>Example Filter Function to remove Design Documents</p>
</blockquote>
<pre class="highlight javascript"><code><span class="kd">function</span><span class="p">(</span><span class="nx">doc</span><span class="p">,</span> <span class="nx">req</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="nx">doc</span><span class="p">.</span><span class="nx">_id</span><span class="p">.</span><span class="nx">indexOf</span><span class="p">(</span><span class="s1">'_design'</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="kc">false</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="kc">true</span><span class="p">;</span>
<span class="p">}</span>
</code></pre>

<p>If you backup design documents, the backup operation creates indexes on the backup destination. This practice slows down the backup process and uses unnecessary amounts of disk space. If you don&rsquo;t require indexes on your backup system, use a <a href="https://docs.cloudant.com/replication_guide.html#filtered-replication">filter function</a> with your replications to filter out design documents. You can also use this filter function to filter out other documents that are not required.</p>

<h4 id="backing-up-multiple-databases">Backing up multiple databases</h4>

<p>If your application uses one database per user, or allows each user to create several databases, you need to create a backup job for each new database.
<aside class='warning'>
Ensure that replication jobs do not begin at the same time.
</aside></p>

<h4 id="local-backups">Local backups</h4>

<p>Cloudant provides a full-featured software version of the Cloudant service, called Cloudant Local. Cloudant Local can be a source or target for replication and is configured in the same manner as <a href="https://docs.cloudant.com/replication_guide.html#how-do-i-initiate-replication-via-the-dashboard">the service-based Cloudant replication.</a>.  Proper configuration requires that Cloudant Local is connected to and is accessible to a network and has a route to and from the Cloudant service.</p>

<p>More information about Cloudant Local can be found at the <a href="https://www.ibm.com/support/knowledgecenter/SSTPQH_1.0.0/com.ibm.cloudant.local.install.doc/topics/clinstall_cloudant_local_overview.html">Cloudant Local documentation site.</a></p>

<h4 id="security-considerations">Security Considerations</h4>

<aside class='notice'>
The following documentation has been extended from its source, found in the <a href='https://docs.cloudant.com/replication_guide.html#permissions'> Permissions</a> and <a href='https://docs.cloudant.com/replication_guide.html#replication-pitfalls'>Pitfalls</a> section of the Replication Guide on the Cloudant documentation site.
</aside>

<p>In order for replication to proceed optimally when replicating from database &ldquo;a&rdquo; to database &ldquo;b&rdquo;, the credentials supplied must have:</p>

<ul>
<li>  <code class="prettyprint">_reader</code> and <code class="prettyprint">_replicator</code> rights on database &ldquo;a&rdquo;.</li>
<li>  <code class="prettyprint">_writer</code> rights on database &ldquo;b&rdquo;.</li>
</ul>

<p>API keys are generated in the Cloudant Dashboard or <a href="https://docs.cloudant.com/authorization.html#creating-api-keys">through the API</a>.</p>

<p>Each key can be given individual rights relating to a specific Cloudant database. Cloudant must be able to write its checkpoint documents at the &ldquo;read&rdquo; end of replication, otherwise no state is saved and replication cannot resume from where it stopped. If the state is not saved, it can lead to performance problems when resuming replications of large data sets. The reason is that without checkpoints, the replication process restarts from the beginning each time it is resumed.</p>

<p>Admin access is required to insert a document into the <code class="prettyprint">_replicator</code> database. The login credentials supplied in the source and target parameters do not require full admin rights. It is sufficient if the credentials are able to:</p>

<ul>
<li>Write documents at the destination end.</li>
<li>Write checkpoint documents at both ends.</li>
</ul>

<p>Cloudant has a special <code class="prettyprint">_replicator</code> user permission. This allows checkpoint documents to be created, but does not allow the creation of ordinary documents in a database. It is recommended that you create API keys that have:</p>

<ul>
<li><code class="prettyprint">_reader</code> and <code class="prettyprint">_replicator</code> access at the source side.</li>
<li><code class="prettyprint">_writer</code> access at the destination side.</li>
</ul>

          <h2 id="rto-and-rpo">RTO and RPO</h2>

<p>The following document contains information about RTO and RPO for the Cloudant: <a href="https://ibm.box.com/v/cloudant-rto">Cloudant RTO</a></p>

          <h1 id="message-hub">Message Hub</h1>

<p>Message Hub is a scalable, distributed, high throughput message bus on the IBM Cloud, available as a fully managed Bluemix service. It is based on Apache Kafka - a fast, scalable, and durable real-time messaging engine from the Apache Software Foundation. To understand the benefits of using Apache Kafka as a service, see <a href="https://developer.ibm.com/messaging/2016/03/14/message-hub-apache-kafka-as-a-service/">Message Hub: Apache Kafka as a Service</a>.</p>

<p>Kafka, and subsequently Message Hub, was designed as a fault-tolerant, distributed system, so Message Hub should remain highly available even during failure conditions. At this time, Message Hub, does not provide the ability to backup or restore message queues or queue metadata. If a catastrophic failure were to occur, applications and application owners will have to re-establish the necessary queues and possibly employ &ldquo;retry logic&rdquo; to ensure a message is sent. </p>

      </div>
      <div class="dark-box">
          <div class="lang-selector">
                <a href="#" data-language-name="javascript">Node</a>
                <a href="#" data-language-name="java">Java</a>
          </div>
      </div>
    </div>
  </body>
</html>
